{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a very simple example to show how to implement deep deterministic policy gradient (ddpg) using pytorch, it only require gym, pytorch and numpy installed to run this notebook, no external files or other libraries is needed, everything needed to work is contained within this notebook. I believe codes written in this way is the most readable\n",
    "\n",
    "-freddy chua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-09 18:25:07,288] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-09 18:25:07,318] Clearing 20 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "env = wrappers.Monitor(env, 'pendulum', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the memory\n",
    "Event = namedtuple('Event', ['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "class Memory(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.buffer = deque(maxlen=self.capacity)\n",
    "\n",
    "  def add_event(self, event):\n",
    "    self.buffer.append(event)\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.buffer, batch_size)\n",
    "\n",
    "# end class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "  def __init__(self, num_states, num_actions):\n",
    "    super(Actor, self).__init__()\n",
    "    self.fc1 = nn.Linear(num_states, 100)\n",
    "    self.fc2 = nn.Linear(100, 50)\n",
    "    self.fc3 = nn.Linear(50, 10)\n",
    "    self.fc4 = nn.Linear(10, num_actions)\n",
    "    \n",
    "    # == parameters initialization ==\n",
    "    nn.init.xavier_normal(self.fc1.weight)\n",
    "    nn.init.xavier_normal(self.fc2.weight)\n",
    "    nn.init.xavier_normal(self.fc3.weight)\n",
    "    nn.init.xavier_normal(self.fc4.weight)\n",
    "    \n",
    "    nn.init.normal(self.fc1.bias)\n",
    "    nn.init.normal(self.fc2.bias)\n",
    "    nn.init.normal(self.fc3.bias)\n",
    "    nn.init.normal(self.fc4.bias)\n",
    "    # =============================== \n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = F.tanh(self.fc4(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "  def __init__(self, num_states, num_actions):\n",
    "    super(Critic, self).__init__()\n",
    "    self.fc1 = nn.Linear(num_states + num_actions, 100)\n",
    "    self.fc2 = nn.Linear(100, 50)\n",
    "    self.fc3 = nn.Linear(50, 10)\n",
    "    self.fc4 = nn.Linear(10, num_actions)\n",
    "    \n",
    "    # == parameters initialization ==\n",
    "    nn.init.xavier_normal(self.fc1.weight)\n",
    "    nn.init.xavier_normal(self.fc2.weight)\n",
    "    nn.init.xavier_normal(self.fc3.weight)\n",
    "    nn.init.xavier_normal(self.fc4.weight)\n",
    "    \n",
    "    nn.init.normal(self.fc1.bias)\n",
    "    nn.init.normal(self.fc2.bias)\n",
    "    nn.init.normal(self.fc3.bias)\n",
    "    nn.init.normal(self.fc4.bias)\n",
    "    # ===============================\n",
    "    \n",
    "  def forward(self, states, actions):\n",
    "    x = torch.cat((states, actions), 1)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_states = 3, num_actions = 1\n"
     ]
    }
   ],
   "source": [
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "print('num_states = {0}, num_actions = {1}'.format(num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_actor = Actor(num_states, num_actions)\n",
    "target_actor = Actor(num_states, num_actions)\n",
    "target_actor.load_state_dict(eval_actor.state_dict())\n",
    "\n",
    "eval_critic = Critic(num_states, num_actions)\n",
    "target_critic = Critic(num_states, num_actions)\n",
    "target_critic.load_state_dict(eval_critic.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100 # for batch processing, larger batch size -> faster computation\n",
    "gamma = 0.99 # the parameter for discounting future rewards\n",
    "tau = 0.001\n",
    "decay = 0.99\n",
    "epsilon = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # mean squared error, similar to least squared error\n",
    "# critic_optimizer = torch.optim.Adam(eval_critic.parameters(), lr=1e-3)\n",
    "# actor_optimizer = torch.optim.Adam(eval_actor.parameters(), lr=1e-4)\n",
    "critic_optimizer = torch.optim.RMSprop(eval_critic.parameters(), lr=1e-3) # RMSprop for learning eval_Q parameters\n",
    "actor_optimizer = torch.optim.RMSprop(eval_actor.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_memory = Memory(100000) # create a replay memory of capacity 10\n",
    "top_score = -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-09 18:25:49,142] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/pendulum/openaigym.video.0.95548.video000000.mp4\n",
      "[2017-07-09 18:25:53,168] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/pendulum/openaigym.video.0.95548.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 reward = -1482.9895357534376 epsilon = 2.0\n",
      "episode 1 reward = -999.5264833621925 epsilon = 1.98\n",
      "episode 2 reward = -1054.7064778529025 epsilon = 1.9602\n",
      "episode 3 reward = -1555.0298248701017 epsilon = 1.9405979999999998\n",
      "episode 4 reward = -1506.1386135469716 epsilon = 1.92119202\n",
      "episode 5 reward = -1386.3748872560438 epsilon = 1.9019800997999998\n",
      "episode 6 reward = -1674.3671643724858 epsilon = 1.8829602988019998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-09 18:26:03,026] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/pendulum/openaigym.video.0.95548.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 7 reward = -1401.6516214026058 epsilon = 1.8641306958139798\n",
      "episode 8 reward = -1295.2689222876625 epsilon = 1.84548938885584\n",
      "episode 9 reward = -1790.4908382002147 epsilon = 1.8270344949672814\n",
      "episode 10 reward = -1589.294724231132 epsilon = 1.8087641500176086\n",
      "episode 11 reward = -1753.6689409536125 epsilon = 1.7906765085174325\n",
      "episode 12 reward = -1747.776494316601 epsilon = 1.7727697434322582\n",
      "episode 13 reward = -1723.1691296879435 epsilon = 1.7550420459979357\n",
      "episode 14 reward = -1514.5198257911109 epsilon = 1.7374916255379562\n",
      "episode 15 reward = -1331.4286795174214 epsilon = 1.7201167092825767\n",
      "episode 16 reward = -1073.3334547447234 epsilon = 1.7029155421897508\n",
      "episode 17 reward = -1763.1204539725031 epsilon = 1.6858863867678533\n",
      "episode 18 reward = -1781.224380530356 epsilon = 1.6690275229001748\n",
      "episode 19 reward = -1242.7950203275168 epsilon = 1.652337247671173\n",
      "episode 20 reward = -1763.6858663119262 epsilon = 1.6358138751944613\n",
      "episode 21 reward = -1266.073773112578 epsilon = 1.6194557364425166\n",
      "episode 22 reward = -1224.2322484358572 epsilon = 1.6032611790780915\n",
      "episode 23 reward = -1758.6196314404956 epsilon = 1.5872285672873105\n",
      "episode 24 reward = -1029.7638521107342 epsilon = 1.5713562816144375\n",
      "episode 25 reward = -1559.3406213180551 epsilon = 1.555642718798293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-09 18:26:27,845] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/pendulum/openaigym.video.0.95548.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 26 reward = -1758.1892216516035 epsilon = 1.5400862916103102\n",
      "episode 27 reward = -1085.683065076302 epsilon = 1.524685428694207\n",
      "episode 28 reward = -1585.181835041169 epsilon = 1.509438574407265\n",
      "episode 29 reward = -1728.5205274326688 epsilon = 1.4943441886631923\n",
      "episode 30 reward = -1138.0561235027947 epsilon = 1.4794007467765604\n",
      "episode 31 reward = -1452.3664932465097 epsilon = 1.4646067393087947\n",
      "episode 32 reward = -1152.7154113783924 epsilon = 1.4499606719157068\n",
      "episode 33 reward = -1150.5253625006974 epsilon = 1.4354610651965496\n",
      "episode 34 reward = -1160.92556411717 epsilon = 1.4211064545445842\n",
      "episode 35 reward = -1000.3722060016518 epsilon = 1.4068953899991383\n",
      "episode 36 reward = -1215.7063904381318 epsilon = 1.392826436099147\n",
      "episode 37 reward = -875.5638397161497 epsilon = 1.3788981717381554\n",
      "episode 38 reward = -987.8194190109926 epsilon = 1.365109190020774\n",
      "episode 39 reward = -1107.314306201686 epsilon = 1.3514580981205662\n",
      "episode 40 reward = -757.251267160954 epsilon = 1.3379435171393605\n",
      "episode 41 reward = -854.7125872108352 epsilon = 1.324564081967967\n",
      "episode 42 reward = -1184.7554887212018 epsilon = 1.3113184411482872\n",
      "episode 43 reward = -871.6307009937462 epsilon = 1.2982052567368043\n",
      "episode 44 reward = -1214.333013989684 epsilon = 1.2852232041694363\n",
      "episode 45 reward = -733.3638503222766 epsilon = 1.2723709721277419\n",
      "episode 46 reward = -1217.8736346255014 epsilon = 1.2596472624064645\n",
      "episode 47 reward = -748.8394751561161 epsilon = 1.2470507897824\n",
      "episode 48 reward = -983.0266087832006 epsilon = 1.234580281884576\n",
      "episode 49 reward = -874.8912521685722 epsilon = 1.2222344790657302\n",
      "episode 50 reward = -913.242900644851 epsilon = 1.210012134275073\n",
      "episode 51 reward = -1092.289569710013 epsilon = 1.1979120129323222\n",
      "episode 52 reward = -905.105655714276 epsilon = 1.1859328928029989\n",
      "episode 53 reward = -1258.0134337162187 epsilon = 1.1740735638749689\n",
      "episode 54 reward = -1153.6967940271834 epsilon = 1.162332828236219\n",
      "episode 55 reward = -646.2413676955158 epsilon = 1.150709499953857\n",
      "episode 56 reward = -1044.532759786252 epsilon = 1.1392024049543183\n",
      "episode 57 reward = -919.0654370090839 epsilon = 1.1278103809047753\n",
      "episode 58 reward = -873.733411557811 epsilon = 1.1165322770957276\n",
      "episode 59 reward = -636.0976272550278 epsilon = 1.1053669543247702\n",
      "episode 60 reward = -1202.7511043377847 epsilon = 1.0943132847815225\n",
      "episode 61 reward = -693.7454626876957 epsilon = 1.0833701519337071\n",
      "episode 62 reward = -756.2601310323778 epsilon = 1.07253645041437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-09 18:27:13,089] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/pendulum/openaigym.video.0.95548.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 63 reward = -513.7579529625892 epsilon = 1.0618110859102263\n",
      "episode 64 reward = -884.439966889101 epsilon = 1.051192975051124\n",
      "episode 65 reward = -737.6229350904517 epsilon = 1.0406810453006128\n",
      "episode 66 reward = -524.7473892618615 epsilon = 1.0302742348476066\n",
      "episode 67 reward = -633.097206565298 epsilon = 1.0199714924991305\n",
      "episode 68 reward = -626.721637522745 epsilon = 1.0097717775741393\n",
      "episode 69 reward = -508.22727337956354 epsilon = 0.9996740597983979\n",
      "episode 70 reward = -526.6642191686301 epsilon = 0.9896773192004139\n",
      "episode 71 reward = -626.1520270189411 epsilon = 0.9797805460084098\n",
      "episode 72 reward = -631.1109279719249 epsilon = 0.9699827405483257\n",
      "episode 73 reward = -495.4945439705794 epsilon = 0.9602829131428424\n",
      "episode 74 reward = -747.6435198080515 epsilon = 0.950680084011414\n",
      "episode 75 reward = -504.7970655102228 epsilon = 0.9411732831712999\n",
      "episode 76 reward = -406.45788516726077 epsilon = 0.9317615503395869\n",
      "episode 77 reward = -496.286909581097 epsilon = 0.922443934836191\n",
      "episode 78 reward = -607.4824199269167 epsilon = 0.9132194954878291\n",
      "episode 79 reward = -762.8088811156136 epsilon = 0.9040873005329508\n",
      "episode 80 reward = -370.9546094120848 epsilon = 0.8950464275276213\n",
      "episode 81 reward = -377.9842848029204 epsilon = 0.8860959632523451\n",
      "episode 82 reward = -250.2724759056009 epsilon = 0.8772350036198217\n",
      "episode 83 reward = -633.827400418373 epsilon = 0.8684626535836234\n",
      "episode 84 reward = -614.9401745163532 epsilon = 0.8597780270477872\n",
      "episode 85 reward = -661.0026509711746 epsilon = 0.8511802467773093\n",
      "episode 86 reward = -1.3340847062987775 epsilon = 0.8426684443095362\n",
      "episode 87 reward = -711.7348850035847 epsilon = 0.8342417598664409\n",
      "episode 88 reward = -126.30456171488818 epsilon = 0.8258993422677765\n",
      "episode 89 reward = -127.0667394723545 epsilon = 0.8176403488450987\n",
      "episode 90 reward = -650.3918670054198 epsilon = 0.8094639453566478\n",
      "episode 91 reward = -497.28873660588147 epsilon = 0.8013693059030813\n",
      "episode 92 reward = -370.50330988878403 epsilon = 0.7933556128440505\n",
      "episode 93 reward = -608.0071331093966 epsilon = 0.78542205671561\n",
      "episode 94 reward = -373.0406379493142 epsilon = 0.7775678361484539\n",
      "episode 95 reward = -128.6650440411166 epsilon = 0.7697921577869694\n",
      "episode 96 reward = -484.72068050942585 epsilon = 0.7620942362090997\n",
      "episode 97 reward = -251.27983089532628 epsilon = 0.7544732938470087\n",
      "episode 98 reward = -254.06957698852347 epsilon = 0.7469285609085385\n",
      "episode 99 reward = -244.0994332803234 epsilon = 0.7394592752994531\n",
      "episode 100 reward = -732.5688726481487 epsilon = 0.7320646825464585\n",
      "episode 101 reward = -250.43594006055747 epsilon = 0.7247440357209939\n",
      "episode 102 reward = -492.1132626645384 epsilon = 0.717496595363784\n",
      "episode 103 reward = -909.0041099482723 epsilon = 0.7103216294101461\n",
      "episode 104 reward = -250.39334101225134 epsilon = 0.7032184131160446\n",
      "episode 105 reward = -503.2149094787427 epsilon = 0.6961862289848841\n",
      "episode 106 reward = -368.1083997746559 epsilon = 0.6892243666950353\n",
      "episode 107 reward = -246.82240902073934 epsilon = 0.6823321230280849\n",
      "episode 108 reward = -126.99781472705138 epsilon = 0.675508801797804\n",
      "episode 109 reward = -251.6554862386383 epsilon = 0.668753713779826\n",
      "episode 110 reward = -126.62288288181267 epsilon = 0.6620661766420277\n",
      "episode 111 reward = -0.8991397359122542 epsilon = 0.6554455148756074\n",
      "episode 112 reward = -470.5845141728607 epsilon = 0.6488910597268513\n",
      "episode 113 reward = -358.5116661004732 epsilon = 0.6424021491295828\n",
      "episode 114 reward = -241.24291635437206 epsilon = 0.635978127638287\n",
      "episode 115 reward = -246.68499529190137 epsilon = 0.6296183463619041\n",
      "episode 116 reward = -122.53914684928525 epsilon = 0.623322162898285\n",
      "episode 117 reward = -127.19319040370934 epsilon = 0.6170889412693021\n",
      "episode 118 reward = -126.9479220308354 epsilon = 0.6109180518566091\n",
      "episode 119 reward = -0.7925978937333957 epsilon = 0.604808871338043\n",
      "episode 120 reward = -609.0762325603586 epsilon = 0.5987607826246625\n",
      "episode 121 reward = -611.1144034854814 epsilon = 0.5927731747984158\n",
      "episode 122 reward = -353.1677110227815 epsilon = 0.5868454430504317\n",
      "episode 123 reward = -237.56030620519113 epsilon = 0.5809769886199274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-09 18:28:24,933] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/pendulum/openaigym.video.0.95548.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 124 reward = -245.65968334156784 epsilon = 0.5751672187337281\n",
      "episode 125 reward = -481.69260722716626 epsilon = 0.5694155465463908\n",
      "episode 126 reward = -123.31414910311966 epsilon = 0.5637213910809269\n",
      "episode 127 reward = -124.25506855832492 epsilon = 0.5580841771701176\n",
      "episode 128 reward = -126.81451514546634 epsilon = 0.5525033353984165\n",
      "episode 129 reward = -633.3362651636518 epsilon = 0.5469783020444323\n",
      "episode 130 reward = -245.31385446992695 epsilon = 0.541508519023988\n",
      "episode 131 reward = -123.63513994427589 epsilon = 0.5360934338337481\n",
      "episode 132 reward = -385.23479261078364 epsilon = 0.5307324994954106\n",
      "episode 133 reward = -1.6686817469031154 epsilon = 0.5254251745004564\n",
      "episode 134 reward = -123.44259565603015 epsilon = 0.5201709227554518\n",
      "episode 135 reward = -126.62581818172012 epsilon = 0.5149692135278974\n",
      "episode 136 reward = -124.8155107411681 epsilon = 0.5098195213926184\n",
      "episode 137 reward = -500.5304745994967 epsilon = 0.5047213261786923\n",
      "episode 138 reward = -123.13483222045187 epsilon = 0.49967411291690533\n",
      "episode 139 reward = -485.7174867057199 epsilon = 0.4946773717877363\n",
      "episode 140 reward = -1.4351448309328008 epsilon = 0.4897305980698589\n",
      "episode 141 reward = -124.07792974536294 epsilon = 0.4848332920891603\n",
      "episode 142 reward = -482.3744181194343 epsilon = 0.4799849591682687\n",
      "episode 143 reward = -124.94429537644885 epsilon = 0.47518510957658605\n",
      "episode 144 reward = -125.86521155057756 epsilon = 0.4704332584808202\n",
      "episode 145 reward = -246.19168529009798 epsilon = 0.465728925896012\n",
      "episode 146 reward = -122.54417452097125 epsilon = 0.46107163663705186\n",
      "episode 147 reward = -359.4242170917093 epsilon = 0.45646092027068136\n",
      "episode 148 reward = -126.02368590078021 epsilon = 0.45189631106797457\n",
      "episode 149 reward = -561.9734643094617 epsilon = 0.44737734795729484\n",
      "episode 150 reward = -124.06293213598778 epsilon = 0.4429035744777219\n",
      "episode 151 reward = -123.33367153830407 epsilon = 0.4384745387329447\n",
      "episode 152 reward = -582.2135774486733 epsilon = 0.4340897933456152\n",
      "episode 153 reward = -125.86831423616667 epsilon = 0.42974889541215905\n",
      "episode 154 reward = -124.37804631025676 epsilon = 0.4254514064580375\n",
      "episode 155 reward = -0.6059399302715234 epsilon = 0.4211968923934571\n",
      "episode 156 reward = -720.6447525986163 epsilon = 0.41698492346952254\n",
      "episode 157 reward = -245.72584982076222 epsilon = 0.41281507423482733\n",
      "episode 158 reward = -0.7244506364373401 epsilon = 0.40868692349247904\n",
      "episode 159 reward = -242.91673873494165 epsilon = 0.40460005425755424\n",
      "episode 160 reward = -245.1141820225255 epsilon = 0.4005540537149787\n",
      "episode 161 reward = -600.7096492217835 epsilon = 0.3965485131778289\n",
      "episode 162 reward = -686.6291514697025 epsilon = 0.3925830280460506\n",
      "episode 163 reward = -121.46514570289638 epsilon = 0.3886571977655901\n",
      "episode 164 reward = -493.26448204384593 epsilon = 0.3847706257879342\n",
      "episode 165 reward = -242.17811807661622 epsilon = 0.38092291953005486\n",
      "episode 166 reward = -125.92485260516148 epsilon = 0.3771136903347543\n",
      "episode 167 reward = -381.312590456056 epsilon = 0.3733425534314067\n",
      "episode 168 reward = -119.32076951871893 epsilon = 0.36960912789709266\n",
      "episode 169 reward = -124.12658821338579 epsilon = 0.36591303661812175\n",
      "episode 170 reward = -240.50640313537968 epsilon = 0.36225390625194054\n",
      "episode 171 reward = -479.0589759165928 epsilon = 0.3586313671894211\n",
      "episode 172 reward = -123.71942508723924 epsilon = 0.3550450535175269\n",
      "episode 173 reward = -361.6076200076703 epsilon = 0.35149460298235163\n",
      "episode 174 reward = -0.8366561771131177 epsilon = 0.34797965695252814\n",
      "episode 175 reward = -125.59193285473042 epsilon = 0.34449986038300284\n",
      "episode 176 reward = -499.5323593232986 epsilon = 0.3410548617791728\n",
      "episode 177 reward = -0.6857768930718615 epsilon = 0.3376443131613811\n",
      "episode 178 reward = -360.15858975832936 epsilon = 0.33426787002976727\n",
      "episode 179 reward = -367.053126917377 epsilon = 0.33092519132946957\n",
      "episode 180 reward = -590.9458256025734 epsilon = 0.3276159394161749\n",
      "episode 181 reward = -240.59435014015443 epsilon = 0.32433978002201314\n",
      "episode 182 reward = -122.15864946640927 epsilon = 0.321096382221793\n",
      "episode 183 reward = -243.42457189537288 epsilon = 0.31788541839957507\n",
      "episode 184 reward = -0.9459725301601699 epsilon = 0.3147065642155793\n",
      "episode 185 reward = -121.08886335876319 epsilon = 0.3115594985734235\n",
      "episode 186 reward = -125.83040057410308 epsilon = 0.3084439035876893\n",
      "episode 187 reward = -124.81843935421105 epsilon = 0.3053594645518124\n",
      "episode 188 reward = -521.3803854546455 epsilon = 0.30230586990629427\n",
      "episode 189 reward = -243.06808723639296 epsilon = 0.2992828112072313\n",
      "episode 190 reward = -236.71845759229643 epsilon = 0.296289983095159\n",
      "episode 191 reward = -123.12292457837287 epsilon = 0.2933270832642074\n",
      "episode 192 reward = -120.54422139303256 epsilon = 0.29039381243156537\n",
      "episode 193 reward = -539.2412370340886 epsilon = 0.2874898743072497\n",
      "episode 194 reward = -243.68244548299265 epsilon = 0.2846149755641772\n",
      "episode 195 reward = -125.32244565868518 epsilon = 0.2817688258085354\n",
      "episode 196 reward = -362.6569476488862 epsilon = 0.27895113755045003\n",
      "episode 197 reward = -123.98753352358801 epsilon = 0.27616162617494555\n",
      "episode 198 reward = -558.3488929766163 epsilon = 0.2734000099131961\n",
      "episode 199 reward = -513.9253534634546 epsilon = 0.27066600981406413\n",
      "episode 200 reward = -125.03094821927496 epsilon = 0.2679593497159235\n",
      "episode 201 reward = -121.52079491209616 epsilon = 0.26527975621876426\n",
      "episode 202 reward = -532.8661910691492 epsilon = 0.2626269586565766\n",
      "episode 203 reward = -245.7048748130899 epsilon = 0.26000068907001084\n",
      "episode 204 reward = -244.35221008166997 epsilon = 0.2574006821793107\n",
      "episode 205 reward = -124.16299965121128 epsilon = 0.25482667535751763\n",
      "episode 206 reward = -494.28194744224874 epsilon = 0.25227840860394246\n",
      "episode 207 reward = -571.4625971964795 epsilon = 0.24975562451790304\n",
      "episode 208 reward = -126.09478810721849 epsilon = 0.247258068272724\n",
      "episode 209 reward = -359.69438748871653 epsilon = 0.24478548758999677\n",
      "episode 210 reward = -123.49657217399901 epsilon = 0.2423376327140968\n",
      "episode 211 reward = -0.3318361269763394 epsilon = 0.23991425638695585\n",
      "episode 212 reward = -239.71720378774177 epsilon = 0.2375151138230863\n",
      "episode 213 reward = -0.23938315627408868 epsilon = 0.23513996268485543\n",
      "episode 214 reward = -496.6274227674389 epsilon = 0.23278856305800688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-09 18:30:16,933] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/pendulum/openaigym.video.0.95548.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 215 reward = -123.6416248685766 epsilon = 0.23046067742742682\n",
      "episode 216 reward = -123.77391728915748 epsilon = 0.22815607065315255\n",
      "episode 217 reward = -237.38425559009272 epsilon = 0.22587450994662103\n",
      "episode 218 reward = -125.59638150409236 epsilon = 0.2236157648471548\n",
      "episode 219 reward = -127.49884287980284 epsilon = 0.22137960719868327\n",
      "episode 220 reward = -352.49714188550854 epsilon = 0.21916581112669645\n",
      "episode 221 reward = -125.22582710323205 epsilon = 0.2169741530154295\n",
      "episode 222 reward = -355.0095194221157 epsilon = 0.2148044114852752\n",
      "episode 223 reward = -0.6252260131325024 epsilon = 0.21265636737042246\n",
      "episode 224 reward = -604.2800714579759 epsilon = 0.21052980369671823\n",
      "episode 225 reward = -123.43261146708127 epsilon = 0.20842450565975104\n",
      "episode 226 reward = -491.1310016383341 epsilon = 0.20634026060315352\n",
      "episode 227 reward = -122.42714747186866 epsilon = 0.20427685799712197\n",
      "episode 228 reward = -119.93383839470025 epsilon = 0.20223408941715076\n",
      "episode 229 reward = -361.9424238589496 epsilon = 0.20021174852297927\n",
      "episode 230 reward = -0.7724720367006513 epsilon = 0.19820963103774947\n",
      "episode 231 reward = -625.5097030072807 epsilon = 0.19622753472737198\n",
      "episode 232 reward = -119.86721764414932 epsilon = 0.19426525938009825\n",
      "episode 233 reward = -126.08397393405872 epsilon = 0.19232260678629726\n",
      "episode 234 reward = -122.75680625546138 epsilon = 0.1903993807184343\n",
      "episode 235 reward = -118.20374490285023 epsilon = 0.18849538691124995\n",
      "episode 236 reward = -125.25443800362207 epsilon = 0.18661043304213745\n",
      "episode 237 reward = -123.34011912522489 epsilon = 0.1847443287117161\n",
      "episode 238 reward = -610.7775557299304 epsilon = 0.18289688542459892\n",
      "episode 239 reward = -124.1698869733328 epsilon = 0.18106791657035293\n",
      "episode 240 reward = -234.87166608802445 epsilon = 0.17925723740464938\n",
      "episode 241 reward = -124.22823892779675 epsilon = 0.17746466503060288\n",
      "episode 242 reward = -360.0414284141096 epsilon = 0.17569001838029685\n",
      "episode 243 reward = -525.4270050737507 epsilon = 0.17393311819649387\n",
      "episode 244 reward = -122.50954547534565 epsilon = 0.17219378701452892\n",
      "episode 245 reward = -124.21170828026492 epsilon = 0.17047184914438362\n",
      "episode 246 reward = -495.3496640701931 epsilon = 0.1687671306529398\n",
      "episode 247 reward = -246.46215211497596 epsilon = 0.1670794593464104\n",
      "episode 248 reward = -359.8106794202438 epsilon = 0.1654086647529463\n",
      "episode 249 reward = -0.09373045728397195 epsilon = 0.16375457810541683\n",
      "episode 250 reward = -486.8026918617772 epsilon = 0.16211703232436267\n",
      "episode 251 reward = -0.42832092658000964 epsilon = 0.16049586200111904\n",
      "episode 252 reward = -239.0396920363104 epsilon = 0.15889090338110784\n",
      "episode 253 reward = -494.0969502215521 epsilon = 0.15730199434729675\n",
      "episode 254 reward = -122.23286157578714 epsilon = 0.15572897440382377\n",
      "episode 255 reward = -0.5637446230314377 epsilon = 0.15417168465978554\n",
      "episode 256 reward = -503.2095314005596 epsilon = 0.1526299678131877\n",
      "episode 257 reward = -369.8277052656141 epsilon = 0.15110366813505582\n",
      "episode 258 reward = -0.35459891513388425 epsilon = 0.14959263145370527\n",
      "episode 259 reward = -124.69029214289817 epsilon = 0.14809670513916823\n",
      "episode 260 reward = -125.5946536018191 epsilon = 0.14661573808777653\n",
      "episode 261 reward = -125.62379219048422 epsilon = 0.14514958070689876\n",
      "episode 262 reward = -355.68372416457214 epsilon = 0.14369808489982977\n",
      "episode 263 reward = -482.54550938845495 epsilon = 0.14226110405083148\n",
      "episode 264 reward = -480.10730453936657 epsilon = 0.14083849301032317\n",
      "episode 265 reward = -244.81625362630157 epsilon = 0.13943010808021994\n",
      "episode 266 reward = -648.137399552521 epsilon = 0.13803580699941773\n",
      "episode 267 reward = -123.70611212004039 epsilon = 0.13665544892942355\n",
      "episode 268 reward = -125.50202020171089 epsilon = 0.13528889444012931\n",
      "episode 269 reward = -364.6517176625992 epsilon = 0.133936005495728\n",
      "episode 270 reward = -369.6985625772319 epsilon = 0.13259664544077074\n",
      "episode 271 reward = -123.78448477076756 epsilon = 0.13127067898636302\n",
      "episode 272 reward = -529.6010929933102 epsilon = 0.12995797219649938\n",
      "episode 273 reward = -124.23963271244422 epsilon = 0.1286583924745344\n",
      "episode 274 reward = -236.43071299879693 epsilon = 0.12737180854978905\n",
      "episode 275 reward = -123.7313978601185 epsilon = 0.12609809046429116\n",
      "episode 276 reward = -123.84124822744236 epsilon = 0.12483710955964825\n",
      "episode 277 reward = -121.90322132624273 epsilon = 0.12358873846405176\n",
      "episode 278 reward = -120.39455949962989 epsilon = 0.12235285107941124\n",
      "episode 279 reward = -355.5154006499298 epsilon = 0.12112932256861712\n",
      "episode 280 reward = -122.66087934816005 epsilon = 0.11991802934293096\n",
      "episode 281 reward = -363.0136056980994 epsilon = 0.11871884904950164\n",
      "episode 282 reward = -125.48376277334299 epsilon = 0.11753166055900663\n",
      "episode 283 reward = -0.42268570450528425 epsilon = 0.11635634395341656\n",
      "episode 284 reward = -239.60780084310161 epsilon = 0.11519278051388239\n",
      "episode 285 reward = -122.12518915811106 epsilon = 0.11404085270874356\n",
      "episode 286 reward = -119.16621821399266 epsilon = 0.11290044418165612\n",
      "episode 287 reward = -479.52521634347636 epsilon = 0.11177143973983956\n",
      "episode 288 reward = -122.8859333183448 epsilon = 0.11065372534244117\n",
      "episode 289 reward = -126.12533333476952 epsilon = 0.10954718808901676\n",
      "episode 290 reward = -121.89538912692873 epsilon = 0.10845171620812659\n",
      "episode 291 reward = -121.22823171273868 epsilon = 0.10736719904604532\n",
      "episode 292 reward = -127.07260662148892 epsilon = 0.10629352705558487\n",
      "episode 293 reward = -354.1672177694957 epsilon = 0.10523059178502901\n",
      "episode 294 reward = -122.21876937293332 epsilon = 0.10417828586717873\n",
      "episode 295 reward = -127.72437858623688 epsilon = 0.10313650300850694\n",
      "episode 296 reward = -124.93402789048208 epsilon = 0.10210513797842187\n",
      "episode 297 reward = -235.56030218131932 epsilon = 0.10108408659863766\n",
      "episode 298 reward = -356.2784326041944 epsilon = 0.10007324573265128\n",
      "episode 299 reward = -359.0665480529003 epsilon = 0.09907251327532476\n",
      "episode 300 reward = -126.0888396582762 epsilon = 0.09808178814257151\n",
      "episode 301 reward = -119.1477372687466 epsilon = 0.0971009702611458\n",
      "episode 302 reward = -125.7026746717623 epsilon = 0.09612996055853433\n",
      "episode 303 reward = -628.270641873499 epsilon = 0.09516866095294899\n",
      "episode 304 reward = -354.1872525900665 epsilon = 0.0942169743434195\n",
      "episode 305 reward = -0.4112744693077852 epsilon = 0.0932748045999853\n",
      "episode 306 reward = -239.26945561105236 epsilon = 0.09234205655398545\n",
      "episode 307 reward = -246.1566348547876 epsilon = 0.09141863598844559\n",
      "episode 308 reward = -371.6976371753013 epsilon = 0.09050444962856113\n",
      "episode 309 reward = -474.3161750652692 epsilon = 0.08959940513227552\n",
      "episode 310 reward = -361.4146457282021 epsilon = 0.08870341108095275\n",
      "episode 311 reward = -125.92335198006617 epsilon = 0.08781637697014323\n",
      "episode 312 reward = -124.24955251859747 epsilon = 0.0869382132004418\n",
      "episode 313 reward = -119.91929050430333 epsilon = 0.08606883106843738\n",
      "episode 314 reward = -472.7476963887044 epsilon = 0.085208142757753\n",
      "episode 315 reward = -554.2458988730867 epsilon = 0.08435606133017547\n",
      "episode 316 reward = -473.7567444181144 epsilon = 0.08351250071687372\n",
      "episode 317 reward = -632.6799405128947 epsilon = 0.08267737570970497\n",
      "episode 318 reward = -121.96813579143209 epsilon = 0.08185060195260792\n",
      "episode 319 reward = -238.34223533249866 epsilon = 0.08103209593308183\n",
      "episode 320 reward = -121.09469067956948 epsilon = 0.08022177497375102\n",
      "episode 321 reward = -123.49277372471049 epsilon = 0.0794195572240135\n",
      "episode 322 reward = -123.92454399399749 epsilon = 0.07862536165177336\n",
      "episode 323 reward = -354.90499860654256 epsilon = 0.07783910803525562\n",
      "episode 324 reward = -354.24603410976016 epsilon = 0.07706071695490306\n",
      "episode 325 reward = -516.6620907616891 epsilon = 0.07629010978535403\n",
      "episode 326 reward = -124.45114964111312 epsilon = 0.07552720868750049\n",
      "episode 327 reward = -505.455531439035 epsilon = 0.07477193660062548\n",
      "episode 328 reward = -119.07701275822949 epsilon = 0.07402421723461922\n",
      "episode 329 reward = -124.022242531177 epsilon = 0.07328397506227302\n",
      "episode 330 reward = -124.70909746267512 epsilon = 0.07255113531165029\n",
      "episode 331 reward = -123.45888353085753 epsilon = 0.07182562395853379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 332 reward = -471.5907114127014 epsilon = 0.07110736771894845\n",
      "episode 333 reward = -125.41438943578281 epsilon = 0.07039629404175896\n",
      "episode 334 reward = -0.29882104221216926 epsilon = 0.06969233110134136\n",
      "episode 335 reward = -126.35122512923421 epsilon = 0.06899540779032795\n",
      "episode 336 reward = -239.05540779175575 epsilon = 0.06830545371242468\n",
      "episode 337 reward = -122.19107919381301 epsilon = 0.06762239917530043\n",
      "episode 338 reward = -127.30316172394991 epsilon = 0.06694617518354742\n",
      "episode 339 reward = -122.90934893000092 epsilon = 0.06627671343171194\n",
      "episode 340 reward = -235.8361169128758 epsilon = 0.06561394629739482\n",
      "episode 341 reward = -472.2648476560204 epsilon = 0.06495780683442087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-09 18:32:43,072] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/pendulum/openaigym.video.0.95548.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 342 reward = -122.64493804262108 epsilon = 0.06430822876607667\n",
      "episode 343 reward = -480.4631814214055 epsilon = 0.0636651464784159\n",
      "episode 344 reward = -493.9431401839924 epsilon = 0.06302849501363174\n",
      "episode 345 reward = -0.6741432296058197 epsilon = 0.06239821006349542\n",
      "episode 346 reward = -364.75856013557336 epsilon = 0.061774227962860466\n",
      "episode 347 reward = -246.57534091394314 epsilon = 0.06115648568323186\n",
      "episode 348 reward = -351.78331008500317 epsilon = 0.06054492082639954\n",
      "episode 349 reward = -364.23785778875185 epsilon = 0.059939471618135544\n",
      "episode 350 reward = -121.44753368234063 epsilon = 0.05934007690195419\n",
      "episode 351 reward = -560.7983553633284 epsilon = 0.05874667613293465\n",
      "episode 352 reward = -123.23829827226972 epsilon = 0.0581592093716053\n",
      "episode 353 reward = -246.7849714439242 epsilon = 0.057577617277889244\n",
      "episode 354 reward = -506.5306591140679 epsilon = 0.05700184110511035\n",
      "episode 355 reward = -348.92271101917015 epsilon = 0.05643182269405925\n",
      "episode 356 reward = -468.4880413124458 epsilon = 0.055867504467118655\n",
      "episode 357 reward = -239.57035970958057 epsilon = 0.05530882942244747\n",
      "episode 358 reward = -126.86530222955561 epsilon = 0.05475574112822299\n",
      "episode 359 reward = -125.46358292935092 epsilon = 0.054208183716940764\n",
      "episode 360 reward = -117.44971870469121 epsilon = 0.053666101879771354\n",
      "episode 361 reward = -124.59688401574958 epsilon = 0.05312944086097364\n",
      "episode 362 reward = -120.81500965595649 epsilon = 0.0525981464523639\n",
      "episode 363 reward = -477.3923019247831 epsilon = 0.052072164987840265\n",
      "episode 364 reward = -493.0279114032377 epsilon = 0.05155144333796186\n",
      "episode 365 reward = -125.65061544767252 epsilon = 0.051035928904582244\n",
      "episode 366 reward = -239.08235545717466 epsilon = 0.05052556961553642\n",
      "episode 367 reward = -241.72074811227762 epsilon = 0.050020313919381054\n",
      "episode 368 reward = -122.15078670139732 epsilon = 0.04952011078018724\n",
      "episode 369 reward = -236.4686753959333 epsilon = 0.04902490967238537\n",
      "episode 370 reward = -125.6516464672574 epsilon = 0.04853466057566151\n",
      "episode 371 reward = -360.52986216631297 epsilon = 0.048049313969904896\n",
      "episode 372 reward = -358.10133092914606 epsilon = 0.047568820830205846\n",
      "episode 373 reward = -123.43922058396682 epsilon = 0.04709313262190379\n",
      "episode 374 reward = -354.7375287050928 epsilon = 0.04662220129568475\n",
      "episode 375 reward = -501.70139421072867 epsilon = 0.0461559792827279\n",
      "episode 376 reward = -572.6050729488858 epsilon = 0.04569441948990063\n",
      "episode 377 reward = -125.28308402826211 epsilon = 0.04523747529500162\n",
      "episode 378 reward = -360.4432843929688 epsilon = 0.044785100542051606\n",
      "episode 379 reward = -120.70720093249622 epsilon = 0.04433724953663109\n",
      "episode 380 reward = -125.43096362665851 epsilon = 0.04389387704126478\n",
      "episode 381 reward = -2.19457362763969 epsilon = 0.04345493827085213\n",
      "episode 382 reward = -479.2641936889358 epsilon = 0.04302038888814361\n",
      "episode 383 reward = -357.17620006737604 epsilon = 0.04259018499926217\n",
      "episode 384 reward = -515.8514039021436 epsilon = 0.042164283149269545\n",
      "episode 385 reward = -125.43080173603555 epsilon = 0.04174264031777685\n",
      "episode 386 reward = -472.84210257934694 epsilon = 0.041325213914599083\n",
      "episode 387 reward = -237.14166769504837 epsilon = 0.040911961775453094\n",
      "episode 388 reward = -125.40420423406893 epsilon = 0.04050284215769856\n",
      "episode 389 reward = -235.4846478510049 epsilon = 0.040097813736121576\n",
      "episode 390 reward = -123.03577563555424 epsilon = 0.03969683559876036\n",
      "episode 391 reward = -124.5329645014714 epsilon = 0.039299867242772756\n",
      "episode 392 reward = -650.0032202296143 epsilon = 0.038906868570345025\n",
      "episode 393 reward = -242.52712787120544 epsilon = 0.038517799884641574\n",
      "episode 394 reward = -499.9973296026754 epsilon = 0.03813262188579516\n",
      "episode 395 reward = -118.26423087940131 epsilon = 0.037751295666937204\n",
      "episode 396 reward = -360.6089469732027 epsilon = 0.03737378271026783\n",
      "episode 397 reward = -0.582398938900335 epsilon = 0.037000044883165155\n",
      "episode 398 reward = -356.9229564649584 epsilon = 0.0366300444343335\n",
      "episode 399 reward = -240.32066960798736 epsilon = 0.03626374398999017\n",
      "episode 400 reward = -125.34923348608147 epsilon = 0.03590110655009027\n",
      "episode 401 reward = -571.0352401368443 epsilon = 0.035542095484589364\n",
      "episode 402 reward = -124.08422766045483 epsilon = 0.03518667452974347\n",
      "episode 403 reward = -234.42233456379105 epsilon = 0.03483480778444604\n",
      "episode 404 reward = -121.35198077915628 epsilon = 0.03448645970660158\n",
      "episode 405 reward = -124.8162147175937 epsilon = 0.034141595109535565\n",
      "episode 406 reward = -246.3054861370998 epsilon = 0.03380017915844021\n",
      "episode 407 reward = -119.30924458767849 epsilon = 0.03346217736685581\n",
      "episode 408 reward = -366.8716552282559 epsilon = 0.03312755559318725\n",
      "episode 409 reward = -473.2309245673425 epsilon = 0.032796280037255376\n",
      "episode 410 reward = -119.85096415592137 epsilon = 0.03246831723688282\n",
      "episode 411 reward = -234.9967167681736 epsilon = 0.032143634064513996\n",
      "episode 412 reward = -480.97089360831666 epsilon = 0.03182219772386886\n",
      "episode 413 reward = -241.58889907196883 epsilon = 0.03150397574663017\n",
      "episode 414 reward = -364.1773226096311 epsilon = 0.03118893598916387\n",
      "episode 415 reward = -120.88962208421323 epsilon = 0.03087704662927223\n",
      "episode 416 reward = -237.93055599959607 epsilon = 0.030568276162979507\n",
      "episode 417 reward = -1.129915845313475 epsilon = 0.03026259340134971\n",
      "episode 418 reward = -121.82103690486584 epsilon = 0.029959967467336215\n",
      "episode 419 reward = -356.93050905574285 epsilon = 0.029660367792662852\n",
      "episode 420 reward = -584.767838645661 epsilon = 0.029363764114736225\n",
      "episode 421 reward = -237.2958207111056 epsilon = 0.02907012647358886\n",
      "episode 422 reward = -470.0465395856956 epsilon = 0.02877942520885297\n",
      "episode 423 reward = -241.1133247922537 epsilon = 0.02849163095676444\n",
      "episode 424 reward = -119.4973251208366 epsilon = 0.028206714647196793\n",
      "episode 425 reward = -360.80963614829955 epsilon = 0.027924647500724827\n",
      "episode 426 reward = -237.5694837882153 epsilon = 0.027645401025717577\n",
      "episode 427 reward = -477.5543846434372 epsilon = 0.0273689470154604\n",
      "episode 428 reward = -122.33373138155456 epsilon = 0.027095257545305798\n",
      "episode 429 reward = -121.41732943949681 epsilon = 0.02682430496985274\n",
      "episode 430 reward = -124.87766308413731 epsilon = 0.026556061920154212\n",
      "episode 431 reward = -119.94317765370808 epsilon = 0.02629050130095267\n",
      "episode 432 reward = -125.88109125057139 epsilon = 0.02602759628794314\n",
      "episode 433 reward = -123.1848030537007 epsilon = 0.02576732032506371\n",
      "episode 434 reward = -499.7465294011102 epsilon = 0.02550964712181307\n",
      "episode 435 reward = -367.250521903299 epsilon = 0.02525455065059494\n",
      "episode 436 reward = -243.69435207407446 epsilon = 0.02500200514408899\n",
      "episode 437 reward = -369.6619333635515 epsilon = 0.0247519850926481\n",
      "episode 438 reward = -124.27049185776914 epsilon = 0.02450446524172162\n",
      "episode 439 reward = -244.1752959729904 epsilon = 0.024259420589304404\n",
      "episode 440 reward = -126.6302959793503 epsilon = 0.02401682638341136\n",
      "episode 441 reward = -125.3875367850298 epsilon = 0.023776658119577247\n",
      "episode 442 reward = -119.04479458568359 epsilon = 0.023538891538381474\n",
      "episode 443 reward = -362.94998615950385 epsilon = 0.02330350262299766\n",
      "episode 444 reward = -525.9954679516753 epsilon = 0.023070467596767685\n",
      "episode 445 reward = -627.80466836422 epsilon = 0.02283976292080001\n",
      "episode 446 reward = -126.44537147587944 epsilon = 0.02261136529159201\n",
      "episode 447 reward = -124.98051184273201 epsilon = 0.02238525163867609\n",
      "episode 448 reward = -125.127049374587 epsilon = 0.02216139912228933\n",
      "episode 449 reward = -119.72887765760612 epsilon = 0.021939785131066435\n",
      "episode 450 reward = -125.78334990183349 epsilon = 0.02172038727975577\n",
      "episode 451 reward = -122.4878455464633 epsilon = 0.021503183406958212\n",
      "episode 452 reward = -359.78357302310343 epsilon = 0.02128815157288863\n",
      "episode 453 reward = -238.98981866927897 epsilon = 0.021075270057159746\n",
      "episode 454 reward = -368.62119564979906 epsilon = 0.020864517356588147\n",
      "episode 455 reward = -501.5628788308181 epsilon = 0.020655872183022266\n",
      "episode 456 reward = -545.1189254289809 epsilon = 0.020449313461192043\n",
      "episode 457 reward = -357.05365002792325 epsilon = 0.02024482032658012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 458 reward = -355.1181632962849 epsilon = 0.02004237212331432\n",
      "episode 459 reward = -236.15131185577835 epsilon = 0.019841948402081176\n",
      "episode 460 reward = -125.83639311414326 epsilon = 0.019643528918060364\n",
      "episode 461 reward = -124.77359373371968 epsilon = 0.01944709362887976\n",
      "episode 462 reward = -125.38934687064555 epsilon = 0.01925262269259096\n",
      "episode 463 reward = -0.32806030683447013 epsilon = 0.01906009646566505\n",
      "episode 464 reward = -124.9250151895693 epsilon = 0.018869495501008398\n",
      "episode 465 reward = -244.6448264259286 epsilon = 0.018680800545998313\n",
      "episode 466 reward = -124.74344100888918 epsilon = 0.01849399254053833\n",
      "episode 467 reward = -583.2805196800344 epsilon = 0.018309052615132947\n",
      "episode 468 reward = -120.36963501157805 epsilon = 0.018125962088981616\n",
      "episode 469 reward = -370.20528827469616 epsilon = 0.0179447024680918\n",
      "episode 470 reward = -124.76140485720842 epsilon = 0.01776525544341088\n",
      "episode 471 reward = -240.8517460925309 epsilon = 0.01758760288897677\n",
      "episode 472 reward = -121.76471072915001 epsilon = 0.017411726860087004\n",
      "episode 473 reward = -242.3539338656451 epsilon = 0.017237609591486135\n",
      "episode 474 reward = -505.4982399004511 epsilon = 0.017065233495571274\n",
      "episode 475 reward = -125.22798056066286 epsilon = 0.01689458116061556\n",
      "episode 476 reward = -240.84734578208423 epsilon = 0.016725635349009404\n",
      "episode 477 reward = -485.63120522012474 epsilon = 0.016558378995519308\n",
      "episode 478 reward = -483.0994935964303 epsilon = 0.016392795205564116\n",
      "episode 479 reward = -0.20055753176949476 epsilon = 0.016228867253508476\n",
      "episode 480 reward = -120.89594403763476 epsilon = 0.016066578580973392\n",
      "episode 481 reward = -240.99393965422448 epsilon = 0.01590591279516366\n",
      "episode 482 reward = -122.63206604858237 epsilon = 0.01574685366721202\n",
      "episode 483 reward = -126.15905027545853 epsilon = 0.015589385130539899\n",
      "episode 484 reward = -125.12667250376873 epsilon = 0.0154334912792345\n",
      "episode 485 reward = -246.7799535903428 epsilon = 0.015279156366442155\n",
      "episode 486 reward = -124.3622333091525 epsilon = 0.015126364802777733\n",
      "episode 487 reward = -375.0157402318422 epsilon = 0.014975101154749955\n",
      "episode 488 reward = -126.02881208210881 epsilon = 0.014825350143202456\n",
      "episode 489 reward = -479.22968001137116 epsilon = 0.01467709664177043\n",
      "episode 490 reward = -122.13130979939417 epsilon = 0.014530325675352727\n",
      "episode 491 reward = -124.94749839461979 epsilon = 0.0143850224185992\n",
      "episode 492 reward = -0.6346294977125024 epsilon = 0.014241172194413208\n",
      "episode 493 reward = -482.1544864852687 epsilon = 0.014098760472469076\n",
      "episode 494 reward = -506.31569204935175 epsilon = 0.013957772867744385\n",
      "episode 495 reward = -482.28545402217026 epsilon = 0.013818195139066942\n",
      "episode 496 reward = -362.113955196535 epsilon = 0.013680013187676273\n",
      "episode 497 reward = -352.3869890250884 epsilon = 0.013543213055799511\n",
      "episode 498 reward = -355.8918256870004 epsilon = 0.013407780925241516\n",
      "episode 499 reward = -0.9841818247643156 epsilon = 0.013273703115989102\n",
      "top_score = -0.09373045728397195\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "  current_state = env.reset() # an array of 4 values\n",
    "  done = False\n",
    "  episode_reward = 0\n",
    "  while not done:    \n",
    "    action = eval_actor(Variable(torch.Tensor(current_state).unsqueeze_(0), volatile=True))\n",
    "    action = torch.squeeze(action.data).numpy() + np.random.randn(1) * epsilon\n",
    "    action = np.maximum(-2.0, np.minimum(action, 2.0)) # this is a domain specific 'hack'\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "      replay_memory.add_event(Event(current_state.copy(), action, None, reward))\n",
    "    else:\n",
    "      replay_memory.add_event(Event(current_state.copy(), action, next_state.copy(), reward))\n",
    "    # end if\n",
    "    current_state = next_state\n",
    "    \n",
    "    # train\n",
    "    if len(replay_memory.buffer) >= batch_size:\n",
    "      # sample from replay memory\n",
    "      mini_batch = replay_memory.sample(batch_size)\n",
    "      mini_batch = Event(*zip(*mini_batch)) # do this for batch processing\n",
    "      \n",
    "      state_var = Variable(torch.Tensor(mini_batch.state))\n",
    "      action_var = Variable(torch.FloatTensor(mini_batch.action))\n",
    "      \n",
    "      estimated_value = eval_critic(state_var, action_var)\n",
    "      \n",
    "      mask = torch.ByteTensor(tuple(map(lambda s: s is not None, mini_batch.next_state)))\n",
    "      \n",
    "      valid_next_states = Variable(torch.Tensor([\n",
    "        next_state for next_state in mini_batch.next_state if next_state is not None]))\n",
    "      \n",
    "      target_val = target_critic(valid_next_states, target_actor(valid_next_states))\n",
    "            \n",
    "      targetted_value = Variable(torch.zeros(batch_size, 1))\n",
    "      targetted_value[mask] = gamma * target_val\n",
    "      targetted_value += Variable(torch.Tensor(mini_batch.reward).unsqueeze_(1))\n",
    "            \n",
    "      # gradient descent on the critic\n",
    "      critic_optimizer.zero_grad()\n",
    "      critic_loss = criterion(estimated_value, targetted_value.detach()) # minimize the mse difference\n",
    "      critic_loss.backward()\n",
    "      critic_optimizer.step()\n",
    "      \n",
    "      # gradient descent on the actor\n",
    "      actor_optimizer.zero_grad()\n",
    "      actor_loss = - eval_critic(state_var, eval_actor(state_var)).mean() # maximize the value of taking action from the policy given by the actor\n",
    "      actor_loss.backward()\n",
    "      actor_optimizer.step()\n",
    "      \n",
    "#       print('critic_loss = {0}, actor_loss = {1}'.format(critic_loss.data[0], actor_loss.data[0]))\n",
    "       \n",
    "      # transfer the parameters from eval to target\n",
    "      for target_param, eval_param in zip(target_critic.parameters(), eval_critic.parameters()):\n",
    "        target_param.data.copy_(tau * eval_param.data + (1 - tau) * target_param.data)\n",
    "      \n",
    "      for target_param, eval_param in zip(target_actor.parameters(), eval_actor.parameters()):\n",
    "        target_param.data.copy_(tau * eval_param.data + (1 - tau) * target_param.data)\n",
    "    # end if\n",
    "  # end while\n",
    "  print('episode {0} reward = {1} epsilon = {2}'.format(i, episode_reward, epsilon))\n",
    "  top_score = max(top_score, episode_reward)\n",
    "  epsilon *= decay\n",
    "# end for\n",
    "print('top_score = {0}'.format(top_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render(close=True)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
