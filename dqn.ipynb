{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a very simple example to show how to implement deep q networks using pytorch, only require gym and pytorch installed, no external files or other libraries, everything needed to work is contained within this notebook\n",
    "\n",
    "-freddy chua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, 'cartpole', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try implementing dqn\n",
    "\n",
    "# the action reward value function can be represented by a mlp\n",
    "class Mlp(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(Mlp, self).__init__() # this statement is always needed\n",
    "    \n",
    "    self.fc1 = nn.Linear(input_size, 10) # matrix multiplication\n",
    "    self.fc2 = nn.Linear(10, output_size) # matrix multiplication\n",
    "    \n",
    "    # == parameters initialization ==\n",
    "    nn.init.xavier_normal(self.fc1.weight)\n",
    "    nn.init.xavier_normal(self.fc2.weight)\n",
    "    \n",
    "    nn.init.normal(self.fc1.bias)\n",
    "    nn.init.normal(self.fc2.bias)\n",
    "    # =============================== \n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "  \n",
    "  # no backward function needed, awesome!\n",
    "# end class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the memory\n",
    "Event = namedtuple('Event', ['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "class Memory(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.idx = 0\n",
    "    self.mem = []\n",
    "\n",
    "  def add_event(self, event):\n",
    "    if len(self.mem) < self.capacity:\n",
    "      self.mem.append(event)\n",
    "    else:\n",
    "      self.mem[self.idx] = event\n",
    "    self.idx = (self.idx + 1) % self.capacity\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.mem, batch_size)\n",
    "\n",
    "# end class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "print('input_size = {0}, output_size = {1}'.format(input_size, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 2 Q-network\n",
    "\n",
    "eval_Q   = Mlp(input_size, output_size)\n",
    "target_Q = Mlp(input_size, output_size)\n",
    "target_Q.load_state_dict(eval_Q.state_dict()) # set them to be similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1.0 # the exploration decision parameter, will decay over time\n",
    "batch_size = 100 # for batch processing, larger batch size -> faster computation\n",
    "gamma = 0.99 # the parameter for discounting future rewards\n",
    "decay = 0.999\n",
    "C = 5 # the time delay in updating target_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(eval_Q.parameters()) # RMSprop for learning eval_Q parameters\n",
    "criterion = nn.MSELoss() # mean squared error, similar to least squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "replay_memory = Memory(10000) # create a replay memory of capacity 10\n",
    "top_score = 0\n",
    "c = 0\n",
    "for i in range(1000):\n",
    "#   print('episode: {0}'.format(i+1))\n",
    "  current_state = env.reset() # an array of 4 values\n",
    "  done = False\n",
    "  episode_reward = 0\n",
    "  while not done:\n",
    "    if random.random() < epsilon:\n",
    "      # perform random action to explore the search space\n",
    "      action = env.action_space.sample()\n",
    "    else:\n",
    "      # choose action with highest value\n",
    "      state_tensor = torch.Tensor(current_state) # convert current_state into a torch tensor\n",
    "      state_tensor = state_tensor.unsqueeze_(0) # unsqueeze to allow for batch processing\n",
    "      # convert to a autograd Variable for automatic backpropagation\n",
    "      state_tensor = Variable(state_tensor, volatile=True) # volatile is True for inference only\n",
    "      action_values = eval_Q(state_tensor) # forward\n",
    "      \n",
    "      _, action = torch.max(action_values, 1)\n",
    "      action = action.data[0,0]\n",
    "    # end if\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "      replay_memory.add_event(Event(current_state.copy(), action, None, reward))\n",
    "    else:\n",
    "      replay_memory.add_event(Event(current_state.copy(), action, next_state.copy(), reward))\n",
    "    # end if\n",
    "    current_state = next_state\n",
    "    \n",
    "    # train\n",
    "    if len(replay_memory.mem) >= batch_size:\n",
    "      # sample from replay memory\n",
    "      mini_batch = replay_memory.sample(batch_size)\n",
    "      mini_batch = Event(*zip(*mini_batch)) # do this for batch processing\n",
    "      \n",
    "      # calculate the estimated value\n",
    "      estimated_value = eval_Q(Variable(torch.Tensor(mini_batch.state)))\n",
    "      # select the value associated with the action taken\n",
    "      estimated_value = estimated_value.gather(1, Variable(torch.LongTensor(mini_batch.action).unsqueeze_(1)))\n",
    "      \n",
    "      # calculate the actual value\n",
    "      mask = torch.ByteTensor(tuple(map(lambda s: s is not None, mini_batch.next_state)))\n",
    "      target_val = target_Q(Variable(torch.Tensor([\n",
    "        next_state for next_state in mini_batch.next_state if next_state is not None])))\n",
    "      target_val, _ = torch.max(target_val, 1)\n",
    "      \n",
    "      targetted_value = Variable(torch.zeros(batch_size, 1))\n",
    "      targetted_value[mask] = gamma * target_val\n",
    "      targetted_value += Variable(torch.Tensor(mini_batch.reward).unsqueeze_(1))\n",
    "      \n",
    "      # compute the loss between estimated value and actual value\n",
    "      optimizer.zero_grad()\n",
    "      loss = criterion(estimated_value, targetted_value.detach())      \n",
    "      loss.backward()\n",
    "      optimizer.step() # do a gradient descent on it\n",
    "      \n",
    "      c += 1\n",
    "      if c == C:\n",
    "        c = 0\n",
    "        target_Q.load_state_dict(eval_Q.state_dict())\n",
    "        epsilon = epsilon * decay\n",
    "      # end if\n",
    "    # end if\n",
    "    \n",
    "  # end while\n",
    "  print('episode {0} reward = {1}, epsilon = {2:3g}'.format(i, episode_reward, epsilon))\n",
    "  top_score = max(top_score, episode_reward)\n",
    "# end for\n",
    "print('top_score = {0}'.format(top_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
