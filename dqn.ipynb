{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a very simple example to show how to implement deep q networks using pytorch, only require gym and pytorch installed, no external files or other libraries, everything needed to work is contained within this notebook\n",
    "\n",
    "-freddy chua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-04 22:18:21,257] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try implementing dqn\n",
    "\n",
    "# the action reward value function can be represented by a mlp\n",
    "class Mlp(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(Mlp, self).__init__() # this statement is always needed\n",
    "    \n",
    "    self.fc1 = nn.Linear(input_size, 10) # matrix multiplication\n",
    "    self.fc2 = nn.Linear(10, output_size) # matrix multiplication\n",
    "    \n",
    "    # == parameters initialization ==\n",
    "    nn.init.xavier_normal(self.fc1.weight)\n",
    "    nn.init.xavier_normal(self.fc2.weight)\n",
    "    \n",
    "    nn.init.normal(self.fc1.bias)\n",
    "    nn.init.normal(self.fc2.bias)\n",
    "    # =============================== \n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "  \n",
    "  # no backward function needed, awesome!\n",
    "# end class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the memory\n",
    "Event = namedtuple('Event', ['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "class Memory(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.idx = 0\n",
    "    self.mem = []\n",
    "\n",
    "  def add_event(self, event):\n",
    "    if len(self.mem) < self.capacity:\n",
    "      self.mem.append(event)\n",
    "    else:\n",
    "      self.mem[self.idx] = event\n",
    "    self.idx = (self.idx + 1) % self.capacity\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.mem, batch_size)\n",
    "\n",
    "# end class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size = 4, output_size = 2\n"
     ]
    }
   ],
   "source": [
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "print('input_size = {0}, output_size = {1}'.format(input_size, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 2 Q-network\n",
    "\n",
    "eval_Q   = Mlp(input_size, output_size)\n",
    "target_Q = Mlp(input_size, output_size)\n",
    "target_Q.load_state_dict(eval_Q.state_dict()) # set them to be similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1.0 # the exploration decision parameter, will decay over time\n",
    "batch_size = 100 # for batch processing, larger batch size -> faster computation\n",
    "gamma = 0.9 # the parameter for discounting future rewards\n",
    "C = 20 # the time delay in updating target_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(eval_Q.parameters()) # RMSprop for learning eval_Q parameters\n",
    "criterion = nn.MSELoss() # mean squared error, similar to least squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 reward = 13.0, epsilon =   1\n",
      "episode 1 reward = 20.0, epsilon =   1\n",
      "episode 2 reward = 12.0, epsilon =   1\n",
      "episode 3 reward = 29.0, epsilon =   1\n",
      "episode 4 reward = 14.0, epsilon =   1\n",
      "episode 5 reward = 12.0, epsilon =   1\n",
      "episode 6 reward = 14.0, epsilon =   1\n",
      "episode 7 reward = 35.0, epsilon = 0.81\n",
      "episode 8 reward = 12.0, epsilon = 0.729\n",
      "episode 9 reward = 41.0, epsilon = 0.59049\n",
      "episode 10 reward = 27.0, epsilon = 0.531441\n",
      "episode 11 reward = 41.0, epsilon = 0.430467\n",
      "episode 12 reward = 39.0, epsilon = 0.348678\n",
      "episode 13 reward = 34.0, epsilon = 0.28243\n",
      "episode 14 reward = 39.0, epsilon = 0.228768\n",
      "episode 15 reward = 46.0, epsilon = 0.185302\n",
      "episode 16 reward = 61.0, epsilon = 0.135085\n",
      "episode 17 reward = 87.0, epsilon = 0.0886294\n",
      "episode 18 reward = 141.0, epsilon = 0.0423912\n",
      "episode 19 reward = 136.0, epsilon = 0.0202756\n",
      "episode 20 reward = 130.0, epsilon = 0.00969774\n",
      "episode 21 reward = 149.0, epsilon = 0.0046384\n",
      "episode 22 reward = 120.0, epsilon = 0.00246503\n",
      "episode 23 reward = 154.0, epsilon = 0.00106112\n",
      "episode 24 reward = 152.0, epsilon = 0.000507529\n",
      "episode 25 reward = 147.0, epsilon = 0.000218475\n",
      "episode 26 reward = 152.0, epsilon = 0.000104496\n",
      "episode 27 reward = 144.0, epsilon = 4.4982e-05\n",
      "episode 28 reward = 138.0, epsilon = 2.15147e-05\n",
      "episode 29 reward = 136.0, epsilon = 1.14338e-05\n",
      "episode 30 reward = 140.0, epsilon = 5.46876e-06\n",
      "episode 31 reward = 132.0, epsilon = 2.61569e-06\n",
      "episode 32 reward = 163.0, epsilon = 1.12597e-06\n",
      "episode 33 reward = 155.0, epsilon = 4.84693e-07\n",
      "episode 34 reward = 162.0, epsilon = 2.08644e-07\n",
      "episode 35 reward = 200.0, epsilon = 7.27497e-08\n",
      "episode 36 reward = 138.0, epsilon = 3.4796e-08\n",
      "episode 37 reward = 200.0, epsilon = 1.21326e-08\n",
      "episode 38 reward = 194.0, epsilon = 4.23038e-09\n",
      "episode 39 reward = 179.0, epsilon = 1.82104e-09\n",
      "episode 40 reward = 194.0, epsilon = 6.34957e-10\n",
      "episode 41 reward = 158.0, epsilon = 2.73328e-10\n",
      "episode 42 reward = 200.0, epsilon = 9.53037e-11\n",
      "episode 43 reward = 154.0, epsilon = 4.10251e-11\n",
      "episode 44 reward = 187.0, epsilon = 1.5894e-11\n",
      "episode 45 reward = 162.0, epsilon = 6.84183e-12\n",
      "episode 46 reward = 158.0, epsilon = 2.94518e-12\n",
      "episode 47 reward = 158.0, epsilon = 1.2678e-12\n",
      "episode 48 reward = 126.0, epsilon = 6.73764e-13\n",
      "episode 49 reward = 113.0, epsilon = 3.58066e-13\n",
      "episode 50 reward = 135.0, epsilon = 1.71262e-13\n",
      "episode 51 reward = 142.0, epsilon = 8.19139e-14\n",
      "episode 52 reward = 134.0, epsilon = 3.91792e-14\n",
      "episode 53 reward = 196.0, epsilon = 1.51788e-14\n",
      "episode 54 reward = 156.0, epsilon = 6.53398e-15\n",
      "episode 55 reward = 139.0, epsilon = 3.12518e-15\n",
      "episode 56 reward = 148.0, epsilon = 1.49477e-15\n",
      "episode 57 reward = 156.0, epsilon = 6.43448e-16\n",
      "episode 58 reward = 180.0, epsilon = 2.49285e-16\n",
      "episode 59 reward = 175.0, epsilon = 9.6578e-17\n",
      "episode 60 reward = 150.0, epsilon = 4.15737e-17\n",
      "episode 61 reward = 143.0, epsilon = 1.98846e-17\n",
      "episode 62 reward = 142.0, epsilon = 9.51072e-18\n",
      "episode 63 reward = 159.0, epsilon = 4.09405e-18\n",
      "episode 64 reward = 196.0, epsilon = 1.42751e-18\n",
      "episode 65 reward = 150.0, epsilon = 6.82773e-19\n",
      "episode 66 reward = 137.0, epsilon = 3.26568e-19\n",
      "episode 67 reward = 183.0, epsilon = 1.26519e-19\n",
      "episode 68 reward = 149.0, epsilon = 6.05137e-20\n",
      "episode 69 reward = 200.0, epsilon = 2.10998e-20\n",
      "episode 70 reward = 156.0, epsilon = 9.08279e-21\n",
      "episode 71 reward = 177.0, epsilon = 3.51886e-21\n",
      "episode 72 reward = 175.0, epsilon = 1.36328e-21\n",
      "episode 73 reward = 153.0, epsilon = 5.86846e-22\n",
      "episode 74 reward = 181.0, epsilon = 2.27356e-22\n",
      "episode 75 reward = 148.0, epsilon = 1.08744e-22\n",
      "episode 76 reward = 123.0, epsilon = 5.77909e-23\n",
      "episode 77 reward = 174.0, epsilon = 2.23894e-23\n",
      "episode 78 reward = 150.0, epsilon = 1.07088e-23\n",
      "episode 79 reward = 183.0, epsilon = 4.1488e-24\n",
      "episode 80 reward = 154.0, epsilon = 1.78592e-24\n",
      "episode 81 reward = 139.0, epsilon = 8.54201e-25\n",
      "episode 82 reward = 197.0, epsilon = 2.97841e-25\n",
      "episode 83 reward = 154.0, epsilon = 1.28211e-25\n",
      "episode 84 reward = 191.0, epsilon = 4.96716e-26\n",
      "episode 85 reward = 200.0, epsilon = 1.73194e-26\n",
      "episode 86 reward = 200.0, epsilon = 6.0389e-27\n",
      "episode 87 reward = 154.0, epsilon = 2.59955e-27\n",
      "episode 88 reward = 170.0, epsilon = 1.11902e-27\n",
      "episode 89 reward = 178.0, epsilon = 4.33531e-28\n",
      "episode 90 reward = 200.0, epsilon = 1.51163e-28\n",
      "episode 91 reward = 170.0, epsilon = 5.85637e-29\n",
      "episode 92 reward = 200.0, epsilon = 2.04199e-29\n",
      "episode 93 reward = 183.0, epsilon = 7.91108e-30\n",
      "episode 94 reward = 10.0, epsilon = 7.91108e-30\n",
      "episode 95 reward = 180.0, epsilon = 3.06492e-30\n",
      "episode 96 reward = 185.0, epsilon = 1.06867e-30\n",
      "episode 97 reward = 185.0, epsilon = 4.14025e-31\n",
      "episode 98 reward = 172.0, epsilon = 1.60402e-31\n",
      "episode 99 reward = 145.0, epsilon = 7.67196e-32\n",
      "episode 100 reward = 125.0, epsilon = 4.07719e-32\n",
      "episode 101 reward = 13.0, epsilon = 3.66948e-32\n",
      "episode 102 reward = 118.0, epsilon = 1.95011e-32\n",
      "episode 103 reward = 164.0, epsilon = 8.39458e-33\n",
      "episode 104 reward = 159.0, epsilon = 3.61359e-33\n",
      "episode 105 reward = 200.0, epsilon = 1.25998e-33\n",
      "episode 106 reward = 200.0, epsilon = 4.39329e-34\n",
      "episode 107 reward = 171.0, epsilon = 1.89117e-34\n",
      "episode 108 reward = 200.0, epsilon = 6.59409e-35\n",
      "episode 109 reward = 175.0, epsilon = 2.55468e-35\n",
      "episode 110 reward = 185.0, epsilon = 9.89737e-36\n",
      "episode 111 reward = 200.0, epsilon = 3.451e-36\n",
      "episode 112 reward = 164.0, epsilon = 1.33699e-36\n",
      "episode 113 reward = 156.0, epsilon = 6.39477e-37\n",
      "episode 114 reward = 154.0, epsilon = 2.75274e-37\n",
      "episode 115 reward = 198.0, epsilon = 9.59821e-38\n",
      "episode 116 reward = 148.0, epsilon = 4.59079e-38\n",
      "episode 117 reward = 153.0, epsilon = 1.97619e-38\n",
      "episode 118 reward = 162.0, epsilon = 8.50683e-39\n",
      "episode 119 reward = 200.0, epsilon = 2.96615e-39\n",
      "episode 120 reward = 200.0, epsilon = 1.03423e-39\n",
      "episode 121 reward = 197.0, epsilon = 3.60614e-40\n",
      "episode 122 reward = 167.0, epsilon = 1.55233e-40\n",
      "episode 123 reward = 162.0, epsilon = 6.68226e-41\n",
      "episode 124 reward = 158.0, epsilon = 2.87649e-41\n",
      "episode 125 reward = 147.0, epsilon = 1.23824e-41\n",
      "episode 126 reward = 133.0, epsilon = 6.58049e-42\n",
      "episode 127 reward = 192.0, epsilon = 2.29448e-42\n",
      "episode 128 reward = 200.0, epsilon = 8.00034e-43\n",
      "episode 129 reward = 200.0, epsilon = 2.78955e-43\n",
      "episode 130 reward = 200.0, epsilon = 9.72655e-44\n",
      "episode 131 reward = 200.0, epsilon = 3.39144e-44\n",
      "episode 132 reward = 175.0, epsilon = 1.31391e-44\n",
      "episode 133 reward = 174.0, epsilon = 5.65596e-45\n",
      "episode 134 reward = 200.0, epsilon = 1.97211e-45\n",
      "episode 135 reward = 200.0, epsilon = 6.87633e-46\n",
      "episode 136 reward = 200.0, epsilon = 2.39763e-46\n",
      "episode 137 reward = 200.0, epsilon = 8.36001e-47\n",
      "episode 138 reward = 200.0, epsilon = 2.91496e-47\n",
      "episode 139 reward = 178.0, epsilon = 1.12931e-47\n",
      "episode 140 reward = 177.0, epsilon = 4.37519e-48\n",
      "episode 141 reward = 185.0, epsilon = 1.69504e-48\n",
      "episode 142 reward = 200.0, epsilon = 5.91024e-49\n",
      "episode 143 reward = 200.0, epsilon = 2.06077e-49\n",
      "episode 144 reward = 200.0, epsilon = 7.18547e-50\n",
      "episode 145 reward = 173.0, epsilon = 2.7838e-50\n",
      "episode 146 reward = 107.0, epsilon = 1.6438e-50\n",
      "episode 147 reward = 200.0, epsilon = 5.73159e-51\n",
      "episode 148 reward = 200.0, epsilon = 1.99848e-51\n",
      "episode 149 reward = 200.0, epsilon = 6.96828e-52\n",
      "episode 150 reward = 165.0, epsilon = 2.69965e-52\n",
      "episode 151 reward = 195.0, epsilon = 1.0459e-52\n",
      "episode 152 reward = 175.0, epsilon = 4.05204e-53\n",
      "episode 153 reward = 186.0, epsilon = 1.56984e-53\n",
      "episode 154 reward = 124.0, epsilon = 7.5085e-54\n",
      "episode 155 reward = 200.0, epsilon = 2.61805e-54\n",
      "episode 156 reward = 200.0, epsilon = 9.12859e-55\n",
      "episode 157 reward = 200.0, epsilon = 3.18294e-55\n",
      "episode 158 reward = 117.0, epsilon = 1.8795e-55\n",
      "episode 159 reward = 12.0, epsilon = 1.69155e-55\n",
      "episode 160 reward = 48.0, epsilon = 1.37015e-55\n",
      "episode 161 reward = 12.0, epsilon = 1.23314e-55\n",
      "episode 162 reward = 11.0, epsilon = 1.10982e-55\n",
      "episode 163 reward = 11.0, epsilon = 1.10982e-55\n",
      "episode 164 reward = 12.0, epsilon = 9.98841e-56\n",
      "episode 165 reward = 126.0, epsilon = 5.30825e-56\n",
      "episode 166 reward = 146.0, epsilon = 2.53892e-56\n",
      "episode 167 reward = 200.0, epsilon = 8.85267e-57\n",
      "episode 168 reward = 186.0, epsilon = 3.08673e-57\n",
      "episode 169 reward = 200.0, epsilon = 1.07628e-57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 170 reward = 95.0, epsilon = 7.06146e-58\n",
      "episode 171 reward = 200.0, epsilon = 2.46218e-58\n",
      "episode 172 reward = 197.0, epsilon = 8.58508e-59\n",
      "episode 173 reward = 200.0, epsilon = 2.99343e-59\n",
      "episode 174 reward = 145.0, epsilon = 1.43175e-59\n",
      "episode 175 reward = 200.0, epsilon = 4.9922e-60\n",
      "episode 176 reward = 169.0, epsilon = 1.93408e-60\n",
      "episode 177 reward = 24.0, epsilon = 1.74067e-60\n",
      "episode 178 reward = 156.0, epsilon = 7.49303e-61\n",
      "episode 179 reward = 145.0, epsilon = 3.58389e-61\n",
      "episode 180 reward = 169.0, epsilon = 1.38847e-61\n",
      "episode 181 reward = 121.0, epsilon = 7.37892e-62\n",
      "episode 182 reward = 105.0, epsilon = 4.35718e-62\n",
      "episode 183 reward = 131.0, epsilon = 2.31558e-62\n",
      "episode 184 reward = 145.0, epsilon = 9.96782e-63\n",
      "episode 185 reward = 48.0, epsilon = 8.07394e-63\n",
      "episode 186 reward = 186.0, epsilon = 3.12801e-63\n",
      "episode 187 reward = 167.0, epsilon = 1.21185e-63\n",
      "episode 188 reward = 139.0, epsilon = 5.79626e-64\n",
      "episode 189 reward = 27.0, epsilon = 5.21664e-64\n",
      "episode 190 reward = 12.0, epsilon = 4.69497e-64\n",
      "episode 191 reward = 109.0, epsilon = 2.77233e-64\n",
      "episode 192 reward = 147.0, epsilon = 1.326e-64\n",
      "episode 193 reward = 160.0, epsilon = 5.70799e-65\n",
      "episode 194 reward = 200.0, epsilon = 1.99025e-65\n",
      "episode 195 reward = 199.0, epsilon = 6.93958e-66\n",
      "episode 196 reward = 147.0, epsilon = 2.98726e-66\n",
      "episode 197 reward = 195.0, epsilon = 1.04159e-66\n",
      "episode 198 reward = 200.0, epsilon = 3.63181e-67\n",
      "episode 199 reward = 139.0, epsilon = 1.9301e-67\n",
      "episode 200 reward = 200.0, epsilon = 6.72983e-68\n",
      "episode 201 reward = 200.0, epsilon = 2.34655e-68\n",
      "episode 202 reward = 153.0, epsilon = 1.01011e-68\n",
      "episode 203 reward = 144.0, epsilon = 4.83133e-69\n",
      "episode 204 reward = 200.0, epsilon = 1.68458e-69\n",
      "episode 205 reward = 191.0, epsilon = 5.87377e-70\n",
      "episode 206 reward = 128.0, epsilon = 3.12156e-70\n",
      "episode 207 reward = 200.0, epsilon = 1.08842e-70\n",
      "episode 208 reward = 167.0, epsilon = 4.21677e-71\n",
      "episode 209 reward = 200.0, epsilon = 1.4703e-71\n",
      "episode 210 reward = 200.0, epsilon = 5.1266e-72\n",
      "episode 211 reward = 169.0, epsilon = 2.20683e-72\n",
      "episode 212 reward = 161.0, epsilon = 9.4997e-73\n",
      "episode 213 reward = 193.0, epsilon = 3.31234e-73\n",
      "episode 214 reward = 162.0, epsilon = 1.42585e-73\n",
      "episode 215 reward = 170.0, epsilon = 6.13783e-74\n",
      "episode 216 reward = 200.0, epsilon = 2.14013e-74\n",
      "episode 217 reward = 200.0, epsilon = 7.46217e-75\n",
      "episode 218 reward = 110.0, epsilon = 3.9657e-75\n",
      "episode 219 reward = 176.0, epsilon = 1.5364e-75\n",
      "episode 220 reward = 176.0, epsilon = 6.61368e-76\n",
      "episode 221 reward = 184.0, epsilon = 2.30605e-76\n",
      "episode 222 reward = 138.0, epsilon = 1.10298e-76\n",
      "episode 223 reward = 200.0, epsilon = 3.84584e-77\n",
      "episode 224 reward = 151.0, epsilon = 1.83945e-77\n",
      "episode 225 reward = 199.0, epsilon = 6.41377e-78\n",
      "episode 226 reward = 145.0, epsilon = 3.06769e-78\n",
      "episode 227 reward = 161.0, epsilon = 1.32054e-78\n",
      "episode 228 reward = 161.0, epsilon = 5.68449e-79\n",
      "episode 229 reward = 164.0, epsilon = 2.20229e-79\n",
      "episode 230 reward = 157.0, epsilon = 1.05335e-79\n",
      "episode 231 reward = 151.0, epsilon = 4.53431e-80\n",
      "episode 232 reward = 162.0, epsilon = 1.95187e-80\n",
      "episode 233 reward = 174.0, epsilon = 7.56195e-81\n",
      "episode 234 reward = 151.0, epsilon = 3.61686e-81\n",
      "episode 235 reward = 123.0, epsilon = 1.72993e-81\n",
      "episode 236 reward = 148.0, epsilon = 8.27421e-82\n",
      "episode 237 reward = 193.0, epsilon = 2.88504e-82\n",
      "episode 238 reward = 171.0, epsilon = 1.24192e-82\n",
      "episode 239 reward = 200.0, epsilon = 4.33029e-83\n",
      "episode 240 reward = 189.0, epsilon = 1.50988e-83\n",
      "episode 241 reward = 196.0, epsilon = 5.84958e-84\n",
      "episode 242 reward = 151.0, epsilon = 2.51805e-84\n",
      "episode 243 reward = 159.0, epsilon = 1.08394e-84\n",
      "episode 244 reward = 195.0, epsilon = 3.77946e-85\n",
      "episode 245 reward = 200.0, epsilon = 1.31782e-85\n",
      "episode 246 reward = 200.0, epsilon = 4.59494e-86\n",
      "episode 247 reward = 186.0, epsilon = 1.78017e-86\n",
      "episode 248 reward = 200.0, epsilon = 6.20709e-87\n",
      "episode 249 reward = 200.0, epsilon = 2.16428e-87\n",
      "episode 250 reward = 152.0, epsilon = 9.3165e-88\n",
      "episode 251 reward = 181.0, epsilon = 3.6094e-88\n",
      "episode 252 reward = 200.0, epsilon = 1.25852e-88\n",
      "episode 253 reward = 152.0, epsilon = 6.01947e-89\n",
      "episode 254 reward = 163.0, epsilon = 2.59118e-89\n",
      "episode 255 reward = 154.0, epsilon = 1.11542e-89\n",
      "episode 256 reward = 159.0, epsilon = 4.80152e-90\n",
      "episode 257 reward = 10.0, epsilon = 4.80152e-90\n",
      "episode 258 reward = 9.0, epsilon = 4.32136e-90\n",
      "episode 259 reward = 15.0, epsilon = 3.88923e-90\n",
      "episode 260 reward = 19.0, epsilon = 3.50031e-90\n",
      "episode 261 reward = 14.0, epsilon = 3.50031e-90\n",
      "episode 262 reward = 157.0, epsilon = 1.50677e-90\n",
      "episode 263 reward = 195.0, epsilon = 5.25377e-91\n",
      "episode 264 reward = 170.0, epsilon = 2.26158e-91\n",
      "episode 265 reward = 200.0, epsilon = 7.88563e-92\n",
      "episode 266 reward = 158.0, epsilon = 3.3945e-92\n",
      "episode 267 reward = 151.0, epsilon = 1.46122e-92\n",
      "episode 268 reward = 180.0, epsilon = 5.66108e-93\n",
      "episode 269 reward = 152.0, epsilon = 2.70768e-93\n",
      "episode 270 reward = 158.0, epsilon = 1.16557e-93\n",
      "episode 271 reward = 188.0, epsilon = 4.06408e-94\n",
      "episode 272 reward = 180.0, epsilon = 1.57451e-94\n",
      "episode 273 reward = 160.0, epsilon = 6.77773e-95\n",
      "episode 274 reward = 178.0, epsilon = 2.62583e-95\n",
      "episode 275 reward = 176.0, epsilon = 1.13033e-95\n",
      "episode 276 reward = 180.0, epsilon = 4.37915e-96\n",
      "episode 277 reward = 182.0, epsilon = 1.52691e-96\n",
      "episode 278 reward = 170.0, epsilon = 6.57287e-97\n",
      "episode 279 reward = 174.0, epsilon = 2.54646e-97\n",
      "episode 280 reward = 200.0, epsilon = 8.87897e-98\n",
      "episode 281 reward = 181.0, epsilon = 3.43989e-98\n",
      "episode 282 reward = 198.0, epsilon = 1.19942e-98\n",
      "episode 283 reward = 152.0, epsilon = 5.73677e-99\n",
      "episode 284 reward = 182.0, epsilon = 2.22254e-99\n",
      "episode 285 reward = 191.0, epsilon = 7.74953e-100\n",
      "episode 286 reward = 168.0, epsilon = 3.33592e-100\n",
      "episode 287 reward = 200.0, epsilon = 1.16316e-100\n",
      "episode 288 reward = 191.0, epsilon = 4.0557e-101\n",
      "episode 289 reward = 200.0, epsilon = 1.41413e-101\n",
      "episode 290 reward = 200.0, epsilon = 4.93078e-102\n",
      "episode 291 reward = 200.0, epsilon = 1.71926e-102\n",
      "episode 292 reward = 200.0, epsilon = 5.99468e-103\n",
      "episode 293 reward = 84.0, epsilon = 3.93311e-103\n",
      "episode 294 reward = 200.0, epsilon = 1.37139e-103\n",
      "episode 295 reward = 161.0, epsilon = 5.90339e-104\n",
      "episode 296 reward = 159.0, epsilon = 2.54121e-104\n",
      "episode 297 reward = 177.0, epsilon = 9.84519e-105\n",
      "episode 298 reward = 115.0, epsilon = 5.23214e-105\n",
      "episode 299 reward = 160.0, epsilon = 2.25226e-105\n",
      "episode 300 reward = 168.0, epsilon = 9.69525e-106\n",
      "episode 301 reward = 138.0, epsilon = 4.63721e-106\n",
      "episode 302 reward = 26.0, epsilon = 4.17349e-106\n",
      "episode 303 reward = 99.0, epsilon = 2.4644e-106\n",
      "episode 304 reward = 165.0, epsilon = 9.5476e-107\n",
      "episode 305 reward = 133.0, epsilon = 5.07399e-107\n",
      "episode 306 reward = 170.0, epsilon = 1.96577e-107\n",
      "episode 307 reward = 115.0, epsilon = 1.16077e-107\n",
      "episode 308 reward = 128.0, epsilon = 5.55191e-108\n",
      "episode 309 reward = 123.0, epsilon = 2.95051e-108\n",
      "episode 310 reward = 200.0, epsilon = 1.02878e-108\n",
      "episode 311 reward = 160.0, epsilon = 4.42856e-109\n",
      "episode 312 reward = 154.0, epsilon = 1.90635e-109\n",
      "episode 313 reward = 135.0, epsilon = 1.01311e-109\n",
      "episode 314 reward = 143.0, epsilon = 4.36112e-110\n",
      "episode 315 reward = 138.0, epsilon = 2.31768e-110\n",
      "episode 316 reward = 200.0, epsilon = 8.08123e-111\n",
      "episode 317 reward = 181.0, epsilon = 2.81775e-111\n",
      "episode 318 reward = 200.0, epsilon = 9.82489e-112\n",
      "episode 319 reward = 150.0, epsilon = 4.69922e-112\n",
      "episode 320 reward = 186.0, epsilon = 1.82057e-112\n",
      "episode 321 reward = 146.0, epsilon = 7.83697e-113\n",
      "episode 322 reward = 126.0, epsilon = 4.16489e-113\n",
      "episode 323 reward = 146.0, epsilon = 1.99205e-113\n",
      "episode 324 reward = 143.0, epsilon = 9.52792e-114\n",
      "episode 325 reward = 113.0, epsilon = 5.06353e-114\n",
      "episode 326 reward = 131.0, epsilon = 2.42187e-114\n",
      "episode 327 reward = 109.0, epsilon = 1.43009e-114\n",
      "episode 328 reward = 100.0, epsilon = 8.44454e-115\n",
      "episode 329 reward = 161.0, epsilon = 3.6351e-115\n",
      "episode 330 reward = 120.0, epsilon = 1.93184e-115\n",
      "episode 331 reward = 107.0, epsilon = 1.14073e-115\n",
      "episode 332 reward = 109.0, epsilon = 6.06232e-116\n",
      "episode 333 reward = 131.0, epsilon = 3.22176e-116\n",
      "episode 334 reward = 107.0, epsilon = 1.71218e-116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 335 reward = 113.0, epsilon = 1.01102e-116\n",
      "episode 336 reward = 100.0, epsilon = 5.97e-117\n",
      "episode 337 reward = 113.0, epsilon = 3.1727e-117\n",
      "episode 338 reward = 136.0, epsilon = 1.51749e-117\n",
      "episode 339 reward = 122.0, epsilon = 8.06458e-118\n",
      "episode 340 reward = 104.0, epsilon = 4.76205e-118\n",
      "episode 341 reward = 101.0, epsilon = 2.81194e-118\n",
      "episode 342 reward = 121.0, epsilon = 1.49438e-118\n",
      "episode 343 reward = 118.0, epsilon = 7.94176e-119\n",
      "episode 344 reward = 123.0, epsilon = 4.22058e-119\n",
      "episode 345 reward = 58.0, epsilon = 3.0768e-119\n",
      "episode 346 reward = 118.0, epsilon = 1.63514e-119\n",
      "episode 347 reward = 183.0, epsilon = 6.33486e-120\n",
      "episode 348 reward = 12.0, epsilon = 5.70137e-120\n",
      "episode 349 reward = 147.0, epsilon = 2.72695e-120\n",
      "episode 350 reward = 97.0, epsilon = 1.61024e-120\n",
      "episode 351 reward = 47.0, epsilon = 1.30429e-120\n",
      "episode 352 reward = 13.0, epsilon = 1.17386e-120\n",
      "episode 353 reward = 19.0, epsilon = 1.05648e-120\n",
      "episode 354 reward = 21.0, epsilon = 9.50829e-121\n",
      "episode 355 reward = 115.0, epsilon = 5.05309e-121\n",
      "episode 356 reward = 152.0, epsilon = 2.41688e-121\n",
      "episode 357 reward = 136.0, epsilon = 1.15599e-121\n",
      "episode 358 reward = 142.0, epsilon = 5.52904e-122\n",
      "episode 359 reward = 131.0, epsilon = 2.64452e-122\n",
      "episode 360 reward = 120.0, epsilon = 1.40541e-122\n",
      "episode 361 reward = 31.0, epsilon = 1.26487e-122\n",
      "episode 362 reward = 146.0, epsilon = 5.44484e-123\n",
      "episode 363 reward = 200.0, epsilon = 1.8985e-123\n",
      "episode 364 reward = 133.0, epsilon = 1.00894e-123\n",
      "episode 365 reward = 155.0, epsilon = 4.34316e-124\n",
      "episode 366 reward = 200.0, epsilon = 1.51436e-124\n",
      "episode 367 reward = 200.0, epsilon = 5.28026e-125\n",
      "episode 368 reward = 124.0, epsilon = 2.80615e-125\n",
      "episode 369 reward = 126.0, epsilon = 1.34217e-125\n",
      "episode 370 reward = 170.0, epsilon = 5.77761e-126\n",
      "episode 371 reward = 157.0, epsilon = 2.48707e-126\n",
      "episode 372 reward = 145.0, epsilon = 1.18956e-126\n",
      "episode 373 reward = 130.0, epsilon = 5.68962e-127\n",
      "episode 374 reward = 138.0, epsilon = 2.72133e-127\n",
      "episode 375 reward = 130.0, epsilon = 1.44623e-127\n",
      "episode 376 reward = 121.0, epsilon = 7.68584e-128\n",
      "episode 377 reward = 143.0, epsilon = 3.67611e-128\n",
      "episode 378 reward = 120.0, epsilon = 1.95364e-128\n",
      "episode 379 reward = 132.0, epsilon = 9.34418e-129\n",
      "episode 380 reward = 136.0, epsilon = 4.46929e-129\n",
      "episode 381 reward = 125.0, epsilon = 2.37517e-129\n",
      "episode 382 reward = 141.0, epsilon = 1.13603e-129\n",
      "episode 383 reward = 171.0, epsilon = 4.40123e-130\n",
      "episode 384 reward = 155.0, epsilon = 2.1051e-130\n",
      "episode 385 reward = 163.0, epsilon = 9.06174e-131\n",
      "episode 386 reward = 181.0, epsilon = 3.15964e-131\n",
      "episode 387 reward = 137.0, epsilon = 1.67916e-131\n",
      "episode 388 reward = 179.0, epsilon = 6.50541e-132\n",
      "episode 389 reward = 153.0, epsilon = 2.80036e-132\n",
      "episode 390 reward = 148.0, epsilon = 1.33941e-132\n",
      "episode 391 reward = 138.0, epsilon = 6.40634e-133\n",
      "episode 392 reward = 136.0, epsilon = 3.06413e-133\n",
      "episode 393 reward = 119.0, epsilon = 1.6284e-133\n",
      "episode 394 reward = 137.0, epsilon = 7.78861e-134\n",
      "episode 395 reward = 146.0, epsilon = 3.72527e-134\n",
      "episode 396 reward = 120.0, epsilon = 1.97976e-134\n",
      "episode 397 reward = 139.0, epsilon = 9.46913e-135\n",
      "episode 398 reward = 200.0, epsilon = 3.30168e-135\n",
      "episode 399 reward = 194.0, epsilon = 1.15123e-135\n",
      "episode 400 reward = 135.0, epsilon = 5.50628e-136\n",
      "episode 401 reward = 148.0, epsilon = 2.63363e-136\n",
      "episode 402 reward = 144.0, epsilon = 1.25966e-136\n",
      "episode 403 reward = 177.0, epsilon = 4.88018e-137\n",
      "episode 404 reward = 119.0, epsilon = 2.59353e-137\n",
      "episode 405 reward = 128.0, epsilon = 1.37831e-137\n",
      "episode 406 reward = 154.0, epsilon = 5.93316e-138\n",
      "episode 407 reward = 200.0, epsilon = 2.06876e-138\n",
      "episode 408 reward = 172.0, epsilon = 8.01481e-139\n",
      "episode 409 reward = 115.0, epsilon = 4.73267e-139\n",
      "episode 410 reward = 137.0, epsilon = 2.26362e-139\n",
      "episode 411 reward = 146.0, epsilon = 9.74414e-140\n",
      "episode 412 reward = 147.0, epsilon = 4.66059e-140\n",
      "episode 413 reward = 182.0, epsilon = 1.80561e-140\n",
      "episode 414 reward = 168.0, epsilon = 7.77256e-141\n",
      "episode 415 reward = 147.0, epsilon = 3.34583e-141\n",
      "episode 416 reward = 154.0, epsilon = 1.6003e-141\n",
      "episode 417 reward = 116.0, epsilon = 8.50465e-142\n",
      "episode 418 reward = 141.0, epsilon = 4.06775e-142\n",
      "episode 419 reward = 174.0, epsilon = 1.57593e-142\n",
      "episode 420 reward = 145.0, epsilon = 7.53762e-143\n",
      "episode 421 reward = 170.0, epsilon = 2.92023e-143\n",
      "episode 422 reward = 180.0, epsilon = 1.13136e-143\n",
      "episode 423 reward = 171.0, epsilon = 4.87012e-144\n",
      "episode 424 reward = 120.0, epsilon = 2.58818e-144\n",
      "episode 425 reward = 109.0, epsilon = 1.37547e-144\n",
      "episode 426 reward = 132.0, epsilon = 7.30979e-145\n",
      "episode 427 reward = 114.0, epsilon = 3.88472e-145\n",
      "episode 428 reward = 200.0, epsilon = 1.35452e-145\n",
      "episode 429 reward = 108.0, epsilon = 7.9983e-146\n",
      "episode 430 reward = 115.0, epsilon = 4.25062e-146\n",
      "episode 431 reward = 141.0, epsilon = 2.03306e-146\n",
      "episode 432 reward = 123.0, epsilon = 1.08045e-146\n",
      "episode 433 reward = 110.0, epsilon = 5.74196e-147\n",
      "episode 434 reward = 125.0, epsilon = 3.05151e-147\n",
      "episode 435 reward = 131.0, epsilon = 1.45953e-147\n",
      "episode 436 reward = 139.0, epsilon = 6.98088e-148\n",
      "episode 437 reward = 118.0, epsilon = 3.70993e-148\n",
      "episode 438 reward = 112.0, epsilon = 2.19068e-148\n",
      "episode 439 reward = 112.0, epsilon = 1.16421e-148\n",
      "episode 440 reward = 25.0, epsilon = 1.04779e-148\n",
      "episode 441 reward = 11.0, epsilon = 9.43014e-149\n",
      "episode 442 reward = 11.0, epsilon = 9.43014e-149\n",
      "episode 443 reward = 11.0, epsilon = 8.48712e-149\n",
      "episode 444 reward = 13.0, epsilon = 8.48712e-149\n",
      "episode 445 reward = 12.0, epsilon = 7.63841e-149\n",
      "episode 446 reward = 96.0, epsilon = 4.51041e-149\n",
      "episode 447 reward = 117.0, epsilon = 2.39701e-149\n",
      "episode 448 reward = 162.0, epsilon = 1.03184e-149\n",
      "episode 449 reward = 180.0, epsilon = 3.99754e-150\n",
      "episode 450 reward = 141.0, epsilon = 1.91201e-150\n",
      "episode 451 reward = 118.0, epsilon = 1.01612e-150\n",
      "episode 452 reward = 200.0, epsilon = 3.543e-151\n",
      "episode 453 reward = 142.0, epsilon = 1.69461e-151\n",
      "episode 454 reward = 150.0, epsilon = 8.10525e-152\n",
      "episode 455 reward = 133.0, epsilon = 3.87671e-152\n",
      "episode 456 reward = 140.0, epsilon = 1.85422e-152\n",
      "episode 457 reward = 131.0, epsilon = 9.85409e-153\n",
      "episode 458 reward = 131.0, epsilon = 4.71318e-153\n",
      "episode 459 reward = 144.0, epsilon = 2.2543e-153\n",
      "episode 460 reward = 150.0, epsilon = 9.70402e-154\n",
      "episode 461 reward = 150.0, epsilon = 4.6414e-154\n",
      "episode 462 reward = 149.0, epsilon = 1.99797e-154\n",
      "episode 463 reward = 129.0, epsilon = 1.0618e-154\n",
      "episode 464 reward = 163.0, epsilon = 4.57072e-155\n",
      "episode 465 reward = 140.0, epsilon = 2.18616e-155\n",
      "episode 466 reward = 134.0, epsilon = 1.04563e-155\n",
      "episode 467 reward = 138.0, epsilon = 5.00123e-156\n",
      "episode 468 reward = 130.0, epsilon = 2.65786e-156\n",
      "episode 469 reward = 139.0, epsilon = 1.27125e-156\n",
      "episode 470 reward = 157.0, epsilon = 5.4723e-157\n",
      "episode 471 reward = 131.0, epsilon = 2.61738e-157\n",
      "episode 472 reward = 123.0, epsilon = 1.39098e-157\n",
      "episode 473 reward = 91.0, epsilon = 9.12625e-158\n",
      "episode 474 reward = 113.0, epsilon = 4.85006e-158\n",
      "episode 475 reward = 168.0, epsilon = 2.08779e-158\n",
      "episode 476 reward = 189.0, epsilon = 7.27969e-159\n",
      "episode 477 reward = 148.0, epsilon = 3.48185e-159\n",
      "episode 478 reward = 126.0, epsilon = 1.66536e-159\n",
      "episode 479 reward = 112.0, epsilon = 9.83378e-160\n",
      "episode 480 reward = 129.0, epsilon = 4.70346e-160\n",
      "episode 481 reward = 135.0, epsilon = 2.49961e-160\n",
      "episode 482 reward = 131.0, epsilon = 1.19556e-160\n",
      "episode 483 reward = 182.0, epsilon = 4.63183e-161\n",
      "episode 484 reward = 200.0, epsilon = 1.61502e-161\n",
      "episode 485 reward = 152.0, epsilon = 6.95214e-162\n",
      "episode 486 reward = 123.0, epsilon = 3.69465e-162\n",
      "episode 487 reward = 128.0, epsilon = 1.96349e-162\n",
      "episode 488 reward = 120.0, epsilon = 1.04348e-162\n",
      "episode 489 reward = 154.0, epsilon = 4.49183e-163\n",
      "episode 490 reward = 177.0, epsilon = 1.74023e-163\n",
      "episode 491 reward = 133.0, epsilon = 9.24828e-164\n",
      "episode 492 reward = 138.0, epsilon = 4.42343e-164\n",
      "episode 493 reward = 200.0, epsilon = 1.54235e-164\n",
      "episode 494 reward = 150.0, epsilon = 6.63932e-165\n",
      "episode 495 reward = 152.0, epsilon = 3.17557e-165\n",
      "episode 496 reward = 131.0, epsilon = 1.51886e-165\n",
      "episode 497 reward = 118.0, epsilon = 8.07187e-166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 498 reward = 143.0, epsilon = 3.86075e-166\n",
      "episode 499 reward = 141.0, epsilon = 1.84658e-166\n",
      "top_score = 200.0\n"
     ]
    }
   ],
   "source": [
    "replay_memory = Memory(10000) # create a replay memory of capacity 10\n",
    "top_score = 0\n",
    "c = 0\n",
    "for i in range(500):\n",
    "#   print('episode: {0}'.format(i+1))\n",
    "  current_state = env.reset() # an array of 4 values\n",
    "  done = False\n",
    "  episode_reward = 0\n",
    "  while not done:\n",
    "    if random.random() < epsilon:\n",
    "      # perform random action to explore the search space\n",
    "      action = env.action_space.sample()\n",
    "    else:\n",
    "      # choose action with highest value\n",
    "      state_tensor = torch.Tensor(current_state) # convert current_state into a torch tensor\n",
    "      state_tensor = state_tensor.unsqueeze_(0) # unsqueeze to allow for batch processing\n",
    "      # convert to a autograd Variable for automatic backpropagation\n",
    "      state_tensor = Variable(state_tensor, volatile=True) # volatile is True for inference only\n",
    "      action_values = eval_Q(state_tensor) # forward\n",
    "      \n",
    "      _, action = torch.max(action_values, 1)\n",
    "      action = action.data[0,0]\n",
    "    # end if\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "      replay_memory.add_event(Event(current_state.copy(), action, None, reward))\n",
    "    else:\n",
    "      replay_memory.add_event(Event(current_state.copy(), action, next_state.copy(), reward))\n",
    "    # end if\n",
    "    current_state = next_state\n",
    "    \n",
    "    # train\n",
    "    if len(replay_memory.mem) >= batch_size:\n",
    "      # sample from replay memory\n",
    "      mini_batch = replay_memory.sample(batch_size)\n",
    "      mini_batch = Event(*zip(*mini_batch)) # do this for batch processing\n",
    "      \n",
    "      # calculate the estimated value\n",
    "      estimated_value = eval_Q(Variable(torch.Tensor(mini_batch.state)))\n",
    "      # select the value associated with the action taken\n",
    "      estimated_value = estimated_value.gather(1, Variable(torch.LongTensor(mini_batch.action).unsqueeze_(1)))\n",
    "      \n",
    "      # calculate the actual value\n",
    "      mask = torch.ByteTensor(tuple(map(lambda s: s is not None, mini_batch.next_state)))\n",
    "      target_val = target_Q(Variable(torch.Tensor([\n",
    "        next_state for next_state in mini_batch.next_state if next_state is not None])))\n",
    "      target_val, _ = torch.max(target_val, 1)\n",
    "      \n",
    "      targetted_value = Variable(torch.zeros(batch_size, 1))\n",
    "      targetted_value[mask] = gamma * target_val\n",
    "      targetted_value += Variable(torch.Tensor(mini_batch.reward).unsqueeze_(1))\n",
    "      \n",
    "      # compute the loss between estimated value and actual value\n",
    "      optimizer.zero_grad()\n",
    "      loss = criterion(estimated_value, targetted_value.detach())      \n",
    "      loss.backward()\n",
    "      optimizer.step() # do a gradient descent on it\n",
    "      \n",
    "      c += 1\n",
    "      if c == C:\n",
    "        c = 0\n",
    "        target_Q.load_state_dict(eval_Q.state_dict())\n",
    "        epsilon = epsilon * 0.9\n",
    "      # end if\n",
    "    # end if\n",
    "    \n",
    "  # end while\n",
    "  print('episode {0} reward = {1}, epsilon = {2:3g}'.format(i, episode_reward, epsilon))\n",
    "  top_score = max(top_score, episode_reward)\n",
    "# end for\n",
    "print('top_score = {0}'.format(top_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
