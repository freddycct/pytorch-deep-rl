{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a very simple example to show how to implement deep q networks (dqn) using pytorch, it only require gym and pytorch installed to run this notebook, no external files or other libraries is needed, everything needed to work is contained within this notebook. I believe codes written in this way is the most readable\n",
    "\n",
    "-freddy chua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple\n",
    "from torch.autograd import Variable\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 10:20:48,262] Making new env: CartPole-v0\n",
      "[2017-07-08 10:20:48,287] Clearing 22 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = wrappers.Monitor(env, 'cartpole', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try implementing dqn\n",
    "\n",
    "# the action reward value function can be represented by a mlp\n",
    "class Mlp(nn.Module):\n",
    "  def __init__(self, input_size, output_size):\n",
    "    super(Mlp, self).__init__() # this statement is always needed\n",
    "    \n",
    "    self.fc1 = nn.Linear(input_size, 10) # matrix multiplication\n",
    "    self.fc2 = nn.Linear(10, output_size) # matrix multiplication\n",
    "    \n",
    "    # == parameters initialization ==\n",
    "    nn.init.xavier_normal(self.fc1.weight)\n",
    "    nn.init.xavier_normal(self.fc2.weight)\n",
    "    \n",
    "    nn.init.normal(self.fc1.bias)\n",
    "    nn.init.normal(self.fc2.bias)\n",
    "    # =============================== \n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "  \n",
    "  # no backward function needed, awesome!\n",
    "# end class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the memory\n",
    "\n",
    "\n",
    "Event = namedtuple('Event', ['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "class Memory(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.idx = 0\n",
    "    self.mem = []\n",
    "\n",
    "  def add_event(self, event):\n",
    "    if len(self.mem) < self.capacity:\n",
    "      self.mem.append(event)\n",
    "    else:\n",
    "      self.mem[self.idx] = event\n",
    "    self.idx = (self.idx + 1) % self.capacity\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.mem, batch_size)\n",
    "\n",
    "# end class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_size = 4, output_size = 2\n"
     ]
    }
   ],
   "source": [
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "print('input_size = {0}, output_size = {1}'.format(input_size, output_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create 2 Q-network\n",
    "\n",
    "eval_Q   = Mlp(input_size, output_size)\n",
    "target_Q = Mlp(input_size, output_size)\n",
    "target_Q.load_state_dict(eval_Q.state_dict()) # set them to be similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1.0 # the exploration decision parameter, will decay over time\n",
    "batch_size = 100 # for batch processing, larger batch size -> faster computation\n",
    "gamma = 0.99 # the parameter for discounting future rewards\n",
    "decay = 0.999\n",
    "C = 5 # the time delay in updating target_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(eval_Q.parameters()) # RMSprop for learning eval_Q parameters\n",
    "criterion = nn.MSELoss() # mean squared error, similar to least squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 10:20:55,919] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/cartpole/openaigym.video.0.83386.video000000.mp4\n",
      "[2017-07-08 10:20:56,808] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/cartpole/openaigym.video.0.83386.video000001.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 reward = 12.0, epsilon =   1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 10:20:57,148] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/cartpole/openaigym.video.0.83386.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 reward = 12.0, epsilon =   1\n",
      "episode 2 reward = 17.0, epsilon =   1\n",
      "episode 3 reward = 13.0, epsilon =   1\n",
      "episode 4 reward = 22.0, epsilon =   1\n",
      "episode 5 reward = 21.0, epsilon =   1\n",
      "episode 6 reward = 22.0, epsilon = 0.996006\n",
      "episode 7 reward = 14.0, epsilon = 0.994015\n",
      "episode 8 reward = 14.0, epsilon = 0.991036\n",
      "episode 9 reward = 23.0, epsilon = 0.986091\n",
      "episode 10 reward = 15.0, epsilon = 0.983135\n",
      "episode 11 reward = 13.0, epsilon = 0.98117\n",
      "episode 12 reward = 15.0, epsilon = 0.978229\n",
      "episode 13 reward = 13.0, epsilon = 0.975298\n",
      "episode 14 reward = 35.0, epsilon = 0.968491\n",
      "episode 15 reward = 22.0, epsilon = 0.964623\n",
      "episode 16 reward = 38.0, epsilon = 0.956933\n",
      "episode 17 reward = 11.0, epsilon = 0.95502\n",
      "episode 18 reward = 43.0, epsilon = 0.946459\n",
      "episode 19 reward = 15.0, epsilon = 0.943623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 10:20:57,777] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/cartpole/openaigym.video.0.83386.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 20 reward = 18.0, epsilon = 0.940795\n",
      "episode 21 reward = 23.0, epsilon = 0.9361\n",
      "episode 22 reward = 29.0, epsilon = 0.930497\n",
      "episode 23 reward = 30.0, epsilon = 0.924928\n",
      "episode 24 reward = 24.0, epsilon = 0.920313\n",
      "episode 25 reward = 43.0, epsilon = 0.912976\n",
      "episode 26 reward = 12.0, epsilon = 0.91024\n",
      "episode 27 reward = 14.0, epsilon = 0.90842\n",
      "episode 28 reward = 18.0, epsilon = 0.904792\n",
      "episode 29 reward = 31.0, epsilon = 0.899377\n",
      "episode 30 reward = 14.0, epsilon = 0.896682\n",
      "episode 31 reward = 17.0, epsilon = 0.893994\n",
      "episode 32 reward = 18.0, epsilon = 0.890424\n",
      "episode 33 reward = 12.0, epsilon = 0.888644\n",
      "episode 34 reward = 23.0, epsilon = 0.884209\n",
      "episode 35 reward = 53.0, epsilon = 0.874531\n",
      "episode 36 reward = 21.0, epsilon = 0.871039\n",
      "episode 37 reward = 33.0, epsilon = 0.865825\n",
      "episode 38 reward = 27.0, epsilon = 0.860643\n",
      "episode 39 reward = 28.0, epsilon = 0.856349\n",
      "episode 40 reward = 30.0, epsilon = 0.851223\n",
      "episode 41 reward = 14.0, epsilon = 0.848672\n",
      "episode 42 reward = 56.0, epsilon = 0.839384\n",
      "episode 43 reward = 87.0, epsilon = 0.824402\n",
      "episode 44 reward = 11.0, epsilon = 0.822754\n",
      "episode 45 reward = 29.0, epsilon = 0.81783\n",
      "episode 46 reward = 18.0, epsilon = 0.815379\n",
      "episode 47 reward = 34.0, epsilon = 0.809689\n",
      "episode 48 reward = 18.0, epsilon = 0.806455\n",
      "episode 49 reward = 23.0, epsilon = 0.803234\n",
      "episode 50 reward = 72.0, epsilon = 0.791269\n",
      "episode 51 reward = 54.0, epsilon = 0.782609\n",
      "episode 52 reward = 19.0, epsilon = 0.780263\n",
      "episode 53 reward = 22.0, epsilon = 0.77637\n",
      "episode 54 reward = 80.0, epsilon = 0.76404\n",
      "episode 55 reward = 130.0, epsilon = 0.744422\n",
      "episode 56 reward = 34.0, epsilon = 0.739226\n",
      "episode 57 reward = 45.0, epsilon = 0.7326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 10:20:59,098] Starting new video recorder writing to /Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/cartpole/openaigym.video.0.83386.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 58 reward = 69.0, epsilon = 0.723133\n",
      "episode 59 reward = 48.0, epsilon = 0.715934\n",
      "episode 60 reward = 56.0, epsilon = 0.708098\n",
      "episode 61 reward = 68.0, epsilon = 0.698249\n",
      "episode 62 reward = 44.0, epsilon = 0.69199\n",
      "episode 63 reward = 11.0, epsilon = 0.690606\n",
      "episode 64 reward = 14.0, epsilon = 0.688537\n",
      "episode 65 reward = 115.0, epsilon = 0.672873\n",
      "episode 66 reward = 16.0, epsilon = 0.670857\n",
      "episode 67 reward = 58.0, epsilon = 0.663514\n",
      "episode 68 reward = 22.0, epsilon = 0.660203\n",
      "episode 69 reward = 100.0, epsilon = 0.647124\n",
      "episode 70 reward = 13.0, epsilon = 0.64583\n",
      "episode 71 reward = 75.0, epsilon = 0.63621\n",
      "episode 72 reward = 87.0, epsilon = 0.624855\n",
      "episode 73 reward = 18.0, epsilon = 0.622983\n",
      "episode 74 reward = 45.0, epsilon = 0.617398\n",
      "episode 75 reward = 63.0, epsilon = 0.60942\n",
      "episode 76 reward = 200.0, epsilon = 0.585513\n",
      "episode 77 reward = 84.0, epsilon = 0.575638\n",
      "episode 78 reward = 23.0, epsilon = 0.573339\n",
      "episode 79 reward = 58.0, epsilon = 0.566497\n",
      "episode 80 reward = 123.0, epsilon = 0.552503\n",
      "episode 81 reward = 60.0, epsilon = 0.545909\n",
      "episode 82 reward = 159.0, epsilon = 0.529237\n",
      "episode 83 reward = 176.0, epsilon = 0.510515\n",
      "episode 84 reward = 81.0, epsilon = 0.502407\n",
      "episode 85 reward = 166.0, epsilon = 0.48609\n",
      "episode 86 reward = 200.0, epsilon = 0.467021\n",
      "episode 87 reward = 183.0, epsilon = 0.450049\n",
      "episode 88 reward = 124.0, epsilon = 0.439371\n",
      "episode 89 reward = 85.0, epsilon = 0.431961\n",
      "episode 90 reward = 189.0, epsilon = 0.415847\n",
      "episode 91 reward = 151.0, epsilon = 0.40355\n",
      "episode 92 reward = 135.0, epsilon = 0.392795\n",
      "episode 93 reward = 200.0, epsilon = 0.377386\n",
      "episode 94 reward = 199.0, epsilon = 0.362581\n",
      "episode 95 reward = 165.0, epsilon = 0.350805\n",
      "episode 96 reward = 194.0, epsilon = 0.337381\n",
      "episode 97 reward = 169.0, epsilon = 0.326097\n",
      "episode 98 reward = 200.0, epsilon = 0.313304\n",
      "episode 99 reward = 200.0, epsilon = 0.301013\n",
      "top_score = 200.0\n"
     ]
    }
   ],
   "source": [
    "replay_memory = Memory(10000) # create a replay memory of capacity 10\n",
    "top_score = 0\n",
    "c = 0\n",
    "for i in range(100):\n",
    "#   print('episode: {0}'.format(i+1))\n",
    "  current_state = env.reset() # an array of 4 values\n",
    "  done = False\n",
    "  episode_reward = 0\n",
    "  while not done:\n",
    "    if random.random() < epsilon:\n",
    "      # perform random action to explore the search space\n",
    "      action = env.action_space.sample()\n",
    "    else:\n",
    "      # choose action with highest value\n",
    "      state_tensor = torch.Tensor(current_state) # convert current_state into a torch tensor\n",
    "      state_tensor = state_tensor.unsqueeze_(0) # unsqueeze to allow for batch processing\n",
    "      # convert to a autograd Variable for automatic backpropagation\n",
    "      state_tensor = Variable(state_tensor, volatile=True) # volatile is True for inference only\n",
    "      action_values = eval_Q(state_tensor) # forward\n",
    "      \n",
    "      _, action = torch.max(action_values, 1)\n",
    "      action = action.data[0,0]\n",
    "    # end if\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "      replay_memory.add_event(Event(current_state.copy(), action, None, reward))\n",
    "    else:\n",
    "      replay_memory.add_event(Event(current_state.copy(), action, next_state.copy(), reward))\n",
    "    # end if\n",
    "    current_state = next_state\n",
    "    \n",
    "    # train\n",
    "    if len(replay_memory.mem) >= batch_size:\n",
    "      # sample from replay memory\n",
    "      mini_batch = replay_memory.sample(batch_size)\n",
    "      mini_batch = Event(*zip(*mini_batch)) # do this for batch processing\n",
    "      \n",
    "      # calculate the estimated value\n",
    "      estimated_value = eval_Q(Variable(torch.Tensor(mini_batch.state)))\n",
    "      # select the value associated with the action taken\n",
    "      estimated_value = estimated_value.gather(1, Variable(torch.LongTensor(mini_batch.action).unsqueeze_(1)))\n",
    "      \n",
    "      # calculate the actual value\n",
    "      mask = torch.ByteTensor(tuple(map(lambda s: s is not None, mini_batch.next_state)))\n",
    "      target_val = target_Q(Variable(torch.Tensor([\n",
    "        next_state for next_state in mini_batch.next_state if next_state is not None])))\n",
    "      target_val, _ = torch.max(target_val, 1)\n",
    "      \n",
    "      targetted_value = Variable(torch.zeros(batch_size, 1))\n",
    "      targetted_value[mask] = gamma * target_val\n",
    "      targetted_value += Variable(torch.Tensor(mini_batch.reward).unsqueeze_(1))\n",
    "      \n",
    "      # compute the loss between estimated value and actual value\n",
    "      optimizer.zero_grad()\n",
    "      loss = criterion(estimated_value, targetted_value.detach())      \n",
    "      loss.backward()\n",
    "      optimizer.step() # do a gradient descent on it\n",
    "      \n",
    "      c += 1\n",
    "      if c == C:\n",
    "        c = 0\n",
    "        target_Q.load_state_dict(eval_Q.state_dict())\n",
    "        epsilon = epsilon * decay\n",
    "      # end if\n",
    "    # end if\n",
    "    \n",
    "  # end while\n",
    "  print('episode {0} reward = {1}, epsilon = {2:3g}'.format(i, episode_reward, epsilon))\n",
    "  top_score = max(top_score, episode_reward)\n",
    "# end for\n",
    "print('top_score = {0}'.format(top_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-08 10:21:10,572] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/fchua/Documents/torch_projects/pytorch_tutorials/pytorch-deep-rl/cartpole')\n"
     ]
    }
   ],
   "source": [
    "env.render(close=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mini_batch = replay_memory.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__new__() takes 5 positional arguments but 101 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-161f93f0a5ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmini_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# do this for batch processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __new__() takes 5 positional arguments but 101 were given"
     ]
    }
   ],
   "source": [
    "mini_batch = Event(*zip(*mini_batch)) # do this for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-0.0868 -0.0374  0.0296  0.0092\n",
       " 0.0254  0.0070 -0.0596 -0.0306\n",
       "-0.3525 -0.3222 -0.0686  0.0349\n",
       "-0.3865 -0.7546 -0.0371 -0.0355\n",
       "-0.2143 -0.3172  0.0003  0.1146\n",
       "-0.0758 -0.9735  0.1966  1.8280\n",
       "-0.0357  0.1839 -0.1127 -0.8558\n",
       "-0.4763 -0.7006 -0.0410  0.3576\n",
       " 0.7877  1.5115  0.1855 -0.7046\n",
       " 0.1615  0.0447 -0.2000 -0.5020\n",
       " 0.0764  0.7824 -0.0922 -1.2270\n",
       "-0.0255 -0.0475  0.0842  0.2408\n",
       "-0.1200  0.1908 -0.0288 -0.8128\n",
       "-0.0091  0.2321 -0.0420 -0.5012\n",
       " 0.0019  0.2054 -0.0639 -0.5254\n",
       " 0.0128  0.2171 -0.0254 -0.2923\n",
       "-0.0186 -0.3655  0.1260  0.8130\n",
       " 0.0624 -0.0375  0.0235  0.2471\n",
       "-0.1513 -0.0071  0.0152 -0.1423\n",
       " 0.0269 -0.1505 -0.0254 -0.0131\n",
       "-0.4416 -0.7626 -0.0319  0.1966\n",
       " 0.6420  0.3702  0.1589  0.4212\n",
       "-0.0660 -0.4253  0.0903  0.6744\n",
       "-0.0597 -0.0117  0.0526  0.0823\n",
       " 0.0106 -0.4159 -0.1215  0.0879\n",
       "-0.0069 -0.2170 -0.0158  0.3112\n",
       "-0.1803 -0.1719  0.0004 -0.0966\n",
       "-0.0844 -0.0382  0.0243  0.0279\n",
       " 0.0004  0.2194 -0.0263 -0.3436\n",
       " 0.4961  1.5143  0.1498 -0.8800\n",
       " 0.0123  0.1645 -0.0962 -0.6823\n",
       "-0.0500 -1.1303  0.1851  1.9487\n",
       "-0.0286  0.2191  0.0413 -0.2020\n",
       "-2.1616 -1.2586 -0.1278 -0.6204\n",
       "-0.1030 -0.3627  0.0194  0.1013\n",
       " 0.0384  0.4050  0.0076 -0.4673\n",
       " 0.0076  0.2333 -0.0592 -0.5057\n",
       " 0.3737 -0.3671 -0.0696  0.7474\n",
       " 0.0507 -0.1784 -0.1146  0.0468\n",
       "-0.0226 -0.6145  0.0311  0.8353\n",
       " 0.0650  0.7938 -0.1761 -1.4751\n",
       "-0.1206 -0.5598 -0.1650 -0.1864\n",
       " 0.0715  1.1435 -0.0803 -1.7303\n",
       " 0.0606 -0.1351 -0.0132  0.1090\n",
       "-0.0398  0.4262 -0.0573 -0.6999\n",
       "-0.0491 -0.2591  0.1875  0.9120\n",
       "-0.2015 -1.3135 -0.0912  0.7602\n",
       " 0.1849 -0.3766 -0.0232  0.7726\n",
       "-0.9735 -0.9334 -0.0647  0.0268\n",
       "-0.1730 -1.1501  0.1113  1.3498\n",
       " 0.1187 -0.0217 -0.1590 -0.2907\n",
       "-0.5809 -0.3069 -0.0304 -0.3036\n",
       "-0.3552  0.2598 -0.0244 -0.7692\n",
       " 0.0482 -0.2046  0.0109  0.3027\n",
       "-0.1549  0.2643 -0.0422 -0.8234\n",
       " 0.0776  0.4018  0.0327 -0.3969\n",
       " 0.0224  0.0369 -0.0449 -0.1408\n",
       "-0.0422 -0.4313  0.1185  0.7376\n",
       "-1.7073 -1.2837 -0.0875 -0.2712\n",
       "-1.3763 -1.4884 -0.1255 -0.1233\n",
       "-0.0349 -0.4030  0.0992  0.6935\n",
       " 0.3363  0.7971 -0.0471 -0.8670\n",
       "-0.0319  0.0522 -0.0215 -0.2031\n",
       "-1.1449 -1.2471 -0.1371  0.1096\n",
       "-1.8565 -1.8041 -0.1495  0.0636\n",
       "-0.1251 -0.7628  0.1810  1.3722\n",
       " 0.0646  0.2091  0.0346 -0.1570\n",
       " 0.0140  0.2115 -0.0071 -0.2092\n",
       "-0.0698 -0.3784 -0.0151  0.2237\n",
       " 0.0933 -0.0467  0.0653  0.2873\n",
       " 0.0004 -0.4165  0.0511  0.6528\n",
       " 0.2020 -0.1630 -0.0860  0.1443\n",
       " 0.0931 -0.5237 -0.0287  0.5808\n",
       "-0.2261 -0.6870 -0.1297 -0.2633\n",
       "-0.4621 -0.3482 -0.0205 -0.2190\n",
       "-0.0285  0.0152  0.0309  0.0143\n",
       "-0.0180 -0.1341 -0.0028  0.0873\n",
       "-0.6947 -0.7049  0.0199  0.1831\n",
       " 0.0407 -0.1750  0.0320  0.4094\n",
       "-0.0116 -0.7751  0.0823  1.4059\n",
       "-0.3253  0.1713  0.0166 -0.6318\n",
       "-0.1160 -0.2085 -0.0330  0.0072\n",
       " 0.1023 -0.1345 -0.0351  0.0178\n",
       "-0.0719  0.0197 -0.0348 -0.2338\n",
       " 0.0225 -0.2233 -0.0626  0.1098\n",
       "-0.0545  0.2384 -0.0446 -0.5759\n",
       "-0.0006 -0.2124 -0.0232  0.2492\n",
       "-0.7686 -0.5559 -0.0314 -0.2787\n",
       "-0.0256  0.0655 -0.0440 -0.3049\n",
       "-0.0068  0.0477 -0.0432 -0.1018\n",
       " 0.0091 -0.3649 -0.0074  0.4724\n",
       " 0.0277  0.0235 -0.0152 -0.0725\n",
       " 0.0443  0.6035 -0.0072 -0.7846\n",
       "-0.0067 -0.1592 -0.0402  0.1084\n",
       "-0.0988 -0.1352 -0.0340 -0.0337\n",
       "-0.0466  0.1363  0.1266  0.3222\n",
       "-0.0360 -0.3801 -0.0268  0.2616\n",
       " 0.1404  0.0210 -0.0060  0.2070\n",
       "-0.1200 -0.2133 -0.0163 -0.1703\n",
       "-0.0102 -0.3787 -0.0253  0.1817\n",
       "[torch.FloatTensor of size 100x4]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(mini_batch.state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
